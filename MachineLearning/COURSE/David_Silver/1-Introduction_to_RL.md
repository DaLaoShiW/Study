# Introduction to RL

---

## 1. About RL

多学科交叉分支，本质上是一门决策科学，研究使用最佳方式决策问题。例如在神经科学中，发现人脑的多巴胺奖励系统本质上也是一种强化学习的决策应用。强化学习是机器学习中的一个重要的分支。

1. 和其他的机器学习算法的区别

   * 不存在监督者，只有奖惩信号
   * 延时反馈
   * 强化学习是时序过程，构建一个和时间有关的动态系统
   * 智能体的行为影响之后的获得的结果(数据)

2. 奖励

   * 标量的反馈信号
   * 显示当前时间步上智能体的好坏
   * 最大化奖励和是强化学习的目标

3. 奖励假设

   **所有的目标都可以被描述成是最大化期望奖励和**

4. 强化学习是一种连续决策问题

   * 选择行动最大化未来的奖励和
   * 长期奖励并且可能延时
   * 需要舍得

5. 智能体和环境

   * agent 本身输出动作
   * agent 观察环境接收信息
   * agent 接收奖励信号
   * env 接收动作更新环境信息
   * env 输出观察信号
   * env 输出奖励信号

## 2. RL Problem

1. 在强化学习过程中历史 history 是观测，动作，奖励的序列，涵盖了 agent 的经验，但是实际上我们完全可以用状态 state 表示这个历史的信息，这个状态决定了下一步的动作的选择等等，是对历史的一种体现。
   $$
   1. \ S_t = f(H_t)\\\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
   2.\ H_t = A_1,O_1,R_1,...,A_t,O_t,R_t\\
   $$

   - $$A,O,R$$: 动作，观察(agent 的输入，对环境的信息的总结)，奖励

   - 状态是历史的函数

   - 状态的分类

     1. 环境状态: 环境的表示，决定了 agent 之后的 action，通常环境状态是 agent 不可视的

     2. agent 状态: 算法执行的基础，agent 状态选择不同的动作，是历史的函数(**对历史信息的某一种总结**)

     3. 信息状态: Markov state

        包含开始到现在所有的有用的信息，下一刻的状态只有当前决定，未来和历史没有关系之和当前有关

        **环境状态和完整历史状态 $$H_t$$ 是 Markov 的**
        $$
        P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]
        $$

2. 环境的观测性

   1. 完全可观测的环境: agent 状态和环境状态一致，MDP

   2. 部分可观测的环境: 

      agent 状态和环境状态不一致，POMDP，这时的  agent 需要构建自己的状态序列获取经验

      - $$S_t = H_t / ...$$

      - 环境状态的信念 $$S_t = (P[S_t=s_1], ..., P[S_t=s_n])$$

        虽然不知道环境的状态，但是可以根据已有经验，利用个体一直状态的概率分布描述当前的个体状态(一个概率向量)

      - 对环境状态建模 RNN (观察当做输入，状态是隐含向量)

## 3. Inside an RL agent

1. agent 内部的主要组件

   * Policy: 行为的函数，状态到动作的函数或者概率函数
     $$
     a=\Pi(s)\\
     \Pi(a|s)=P[A=a|S=s]
     $$

   * Value function: 状态或者动作的价值，对未来奖励的预测是基于 Policy
     $$
     V_{\pi}=E_{\pi}[R_t + \gamma R_{t+1} + \gamma^2R_{t+2} + ... | S_t=s]
     $$

   * Model: 对环境的建模表示，agent 对环境的感知，agent 预测环境而构建的模型

     预测下一个状态的概率，预测奖励信号的大小

2. agent 的分类

   1. Value Based No Policy
   2. Policy Based No Value
   3. Actor Critic, combination above
   4. Model Free without model, only focus on value and policy
   5. Model Based 

## 4. Problems with RL

1. RL
   * 环境未知
   * agent 和环境交互
   * 交互过程中提升 Policy
2. Planning
   * 环境已知，所有的规则已知model 提前供给
   * 不需要实际交互，离线学习，利用环境模型计算
   * 提升 Policy
   * 平衡 exploration 和 exploitation