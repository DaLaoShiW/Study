## 1. 任务和奖赏

  1.  强化学习通常用 MDP 马尔可夫决策过程描述 $$S,A,P_{ss'}^a,R_s^a,\gamma$$

      在有的应用中，奖励 $$R$$ 之和状态之间的转移有关，和动作没有关系

		2. 在环境中，状态转移，奖赏的返回不受机器控制，agent 只能通过动作影响环境并观察到部分信息

		3. agent 的目的在于在环境那个中不断尝试学习一个策略 $$\pi$$ ，策略可以是确定性的也可以是不确定的，策略的优劣通过累计奖励的方式进行衡量，长期累计奖励有很多计算方式

     * T 步累计奖赏期望: $$\mathbb{E}[\frac{1}{T}\sum_{t=1}^tr_t]$$
     * $$\gamma$$ 折扣累计奖励: $$\mathbb{E}[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}]$$

## 2. K-摇臂赌博机

K-摇臂赌博机对应的是但不强化学习，意味着最大化单步奖励，但是每一个动作的奖赏是未知的概率分布。

在这种情况下，尝试次数有限，探索和利用需要进行平衡才可以获得最到的累计奖赏。平衡的方式如下

1. $$\epsilon$$-greedy

   $$\epsilon$$ 的概率探索(随机尝试),$$1-\epsilon$$ 的概率利用(选择当前平均奖赏最高的动作)

   $$Q_n(k)=\frac{1}{n}\sum_{i=1}^nv_i=Q_{n-1}(k) + \frac{1}{n}(v_n-Q_{n-1}(k))$$

2. softmax
   $$
   P(k)=\frac{e^{\frac{Q(k)}{\eta}}}{\sum_{i=1}^ke^{\frac{Q(i)}{\eta}}}
   $$

   * $$\eta > 0$$ : 温度系数，越小越容易选择平均奖赏高的动作，越大趋于仅探索
   * 计算不同的动作的选择概率

在强化学习中，将所有的状态上的动作选择过程看成一个 K-摇臂赌博机问题，使用强化学习的累计奖励替代这里的奖励 $$v$$

## 3. 有模型学习

对于 MDP $$(S,A,P,R)$$ 均已知，这种情况下我们称之为模型已知

1. 策略评估

   已知模型和对应的策略 $$\pi$$ ，计算该策略可以带来的最大累计期望 

   * $$V^{\pi}(x)$$: 从状态 $$x$$ 出发使用策略 $$\pi$$ 得到的累计奖励，**状态价值函数**

     T 步
     $$
     V^{\pi}_T(x)=\mathbb{E}_{\pi}[\frac{1}{T}\sum_{t=1}^Tr_t|x_0=x]
     $$
     折扣
     $$
     V^{\pi}_\gamma(x)=\mathbb{E}_{\pi}[\sum_{t=0}^{+\infty}\gamma^t r_{t+1}|x_0=x]
     $$
     ​

   * $$Q^{\pi}(x, a)$$: 从状态 $$x$$ 出发使用动作 $$a$$ 后在策略 $$\pi$$ 指导下带来的累计奖励，**状态动作价值函数**

     T 步
     $$
     Q_T^{\pi}(x)=\mathbb{E}_{\pi}[\frac{1}{T}\sum_{t=1}^Tr^t|x_0=x,a_0=a]
     $$
     折扣
     $$
     Q_\gamma^{\pi}(x)=\mathbb{E}_{\pi}[\sum_{t=0}^{+\infty}\gamma^tr_{t+1}|x_0=x,a_0=a]
     $$


   上面的式子均可以使用 Bellman 公式全概率展开($$P,R$$ 已知的情况下)

   策略评估算法

   ```bash
   MDP E=<S,A,P,R>
       policy=pi
       
   for all x: V(x)=0
   for t=1,2,... do
   	for all x: V'(x)=Bellman 全概率展开的计算公式
   	if t = T + 1 then 也可以使用精度限制的方式决定是否退出计算
   	    break
   	else
   		V(x) = V'(x)
   	end if
   end for
   ```

   状态价值函数得到了，状态动作价值函数也可以计算得到s

2. 策略改进

   $$\pi*=\arg \max_{\pi}\sum_{x\in X}V^{\pi}(x)$$

   参见 David Silver 的讲义

## 免模型学习

在免模型学习的情况下，策略迭代算法无法对策略进行评估，因为无法展开全概率公式，只能在环境中执行选择的动作观察转移状态得到的奖赏。

1. MC

   梦特卡罗算法是采用采样的方式，计算平均累计将上作为期望的近似。因为在有模型学习中，知道了 $$V$$ 也就知道了 $$Q$$ ，但是这里我们必须只能知道 $$Q$$ 才可以得到结果

2. TD 学习