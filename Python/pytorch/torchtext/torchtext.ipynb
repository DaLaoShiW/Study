{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchtext and spacy for preprocessing for NLP\n",
    "1. `spacy` is the wonderful processing library for NLP\n",
    "2. `Torchtext` is the wonderful dataset creatation tools for `pytorch` or other frame (`tensorflow`, ...)\n",
    "3. Functionalities for `Torchtext`\n",
    "    * File Loading: Load the corpus from various framous formats\n",
    "    * Tokenization: `spacy` can be more powerful\n",
    "    * Vocab: generator a vocabulary list\n",
    "    * Numericalize / Indexify: Map words into integer numbers of index in the entire corpus\n",
    "    * Word Vector: randomly initialize or pre-train \n",
    "    * Batching: padding is normally happening here\n",
    "4. refer\n",
    "    * [Torchtext document](https://torchtext.readthedocs.io/en/latest/data.html#fields)\n",
    "    * [Torchtext chinese blog](https://somewayqxq.com/2017/torchtext-doc/)\n",
    "    * [Torchtext foreign blog](http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)\n",
    "    * [Allen NLP Torchtext](http://anie.me/On-Torchtext/)\n",
    "    * [My repo](https://github.com/gmftbyGMFTBY/MiniNMT)\n",
    "    * [GitHub practice for torchtext](https://github.com/keitakurita/practical-torchtext)\n",
    "    * [GitHub Tutorial for Torchtext](https://github.com/mjc92/TorchTextTutorial)\n",
    "    \n",
    "![](./torchtext.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy and load the english\n",
    "# We only use spacy's tokenizer function here\n",
    "import spacy\n",
    "spacy_en = spacy.load(\"en\")\n",
    "\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline for this tutorial \n",
    "* **Field**\n",
    "* **Dataset**\n",
    "* **Example**\n",
    "* **Iterator**\n",
    "* **Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Field**\n",
    "```python\n",
    "data.Field(sequential=True, use_vocab=True, init_token=None, \n",
    "           eos_token=None, fix_length=None, dtype=torch.int64,\n",
    "           preprocessing=None, postprocessing=None, \n",
    "           lower=False, tokenize=fucntion instance, \n",
    "           include_lengths=False, batch_forst=False, \n",
    "           pad_token='<pad>', unk_token='<unk>', \n",
    "           pad_first=False, stop_words=None, ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `sequential`:  \n",
    "这个域的数据是否是序列，如果是序列使用 `tokenize` 函数进行分割，否则视为是数字等其他的数据类型，默认是 `True`\n",
    "* `use_vocab`:  \n",
    "这个域 (`Field`) 的数据是否是一个词典对象，`False` 说明数据已经是数字类型，默认是 `True`\n",
    "* `init_token, eos_token, pad_token, unk_token`:  \n",
    "特殊 `token`, 其中 `init_token, eos_token` 需要指定，`pad_token, unk_token` 可以使用默认值\n",
    "* `fix_length`:  \n",
    "`padding` 指定的特定长度\n",
    "* `dtype`:  \n",
    "指定数据类型，默认是 `torch.long`\n",
    "* `preprocessing`:  \n",
    "这个预处理管道函数用来在 `tokenizer` 之后 `numericalizing` 之前执行，用于自定义的预处理\n",
    "* `postprocessing`:  \n",
    "在 `numericaling` 之后 `number` 转换成 `tensor` 之前执行，输入是 `batch`\n",
    "* `lower`:  \n",
    "是否将输入的 `text` 全部小写化，默认是 `False`\n",
    "* `tokenize`:  \n",
    "用来 `tokenize` 输入的文本，默认是 `str.split` 一般替换成 `spacy`\n",
    "* `include_lengths`:  \n",
    "`True` 返回的数据是 `(padded minibatch, lengths of each examples)`, `False` 返回的是 `padded minibatch`\n",
    "* `batch_first`:  \n",
    "`False` 默认，`shape` 是 `[, batch]`, `True shape, [batch, ]`\n",
    "* `build_vocab(train, vectors, min_freq, max_size)`:  \n",
    "该方法是用来在该 `Field` 上生成对应的词典，需要使用 `train / test/ valid` 数据集对象，这个我们之后会提到，在这个数据集对象上建立对应的词典, `vectors` 指定预训练的词向量 (`glove.6B.100d`).`min_freq` 规定了词表中的最小的单词频率，过滤掉了出现频次小的单词,`max_size` 规定了词表的最大大小，如果超过了的话，按照出现的频次选取 `top-k` 个，频率最前面的单词会出现在词表中。\n",
    "* `vocab`:  \n",
    "这是一个字典，这个字典在 `build_vocab` 函数调用之后会得到，这个字典最有名的方法就是\n",
    "    * `length`: \n",
    "    ```python\n",
    "    len(Field.vocab)\n",
    "    ```\n",
    "    * `itos`:\n",
    "    ```python\n",
    "    # index to string\n",
    "    Field.itos[0]\n",
    "    ```\n",
    "    * `stoi`:\n",
    "    ```python\n",
    "    Field.stoi['string']\n",
    "    ```\n",
    "    * `vectors`:  \n",
    "    当 `build_vaocb` 的时候使用了预训练的词向量的时候这一项就是词向量的 `lookup table` 之后可以赋值给 `torch.Embedding`\n",
    "    * `freqs`:  \n",
    "    对 `vocab` 中的数据统计出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<unk>', torch.int64, True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    # use spacy tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# This case is from `text-classification` file\n",
    "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "TEXT.pad_token, TEXT.unk_token, TEXT.dtype, TEXT.use_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. **Dataset**\n",
    "`Dataset` 对象是 `torchtext` 中非常重要的一个对象，主要分为自定义 `Dataset` 和 `TabularDataset`\n",
    "1. `TabularDataset`  \n",
    "从 `CSV, TSV, JSON` 数据存储格式中加载对应使用 `columns` 存储的数据\n",
    "```python\n",
    "data.TabularDataset(path, format, fields, skip_header=False, ...)\n",
    "```\n",
    "    * `path`:  \n",
    "数据文件的路径\n",
    "    * `format(str)`:  \n",
    "数据文件的存储格式 `csv, tsv, json`\n",
    "    * `fields`:  \n",
    "    ```python\n",
    "    [('field_name', field_object), ...]\n",
    "    ```\n",
    "    使用的是 `list` 只能是 `csv, tsv` 格式的文件，并且和文件内部的列对应\n",
    "    ```python\n",
    "    dict[str: tuple(str, Field)]\n",
    "    ```\n",
    "    如果是字典类型，`key` 是 `JSON / CSV / TSV` 中的键名或者列名\n",
    "    * `skip_header(bool)`:  \n",
    "    是否跳过首行的文本，比如 `CSV, TSV` 文件中的列说明\n",
    "    * `splits(path, train, validation, test, format, fields)`:  \n",
    "    根据下面的具体实例可以看出，该方法用来对 `test, validation, train` 数据集进行对应的分割生成对应的**数据集对象**\n",
    "    \n",
    "2. `Dataset`  \n",
    "使用 `Examples` 对象构成的列表创建 `dataset` 对象，自定义程度更高\n",
    "```python\n",
    "data.Dataset(examples, fields, filter_pred=None)\n",
    "```\n",
    "    * `examples`:   \n",
    "    `Example` 对象的列表，之后会提到\n",
    "    * `fields`:  \n",
    "    ```python\n",
    "    list(tuple(str, Field))\n",
    "    ```\n",
    "    * `filter_pred(function or None)`:  \n",
    "    当 `Example` 对象经过过滤函数 `filter_pred` 函数是 `True` 的时候允许加入数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'what': 2,\n",
       "         'the': 1,\n",
       "         'fuck': 2,\n",
       "         'are': 2,\n",
       "         'you': 10,\n",
       "         'doing': 1,\n",
       "         'here': 2,\n",
       "         '!': 3,\n",
       "         'really': 3,\n",
       "         'piss': 1,\n",
       "         'me': 1,\n",
       "         'off': 1,\n",
       "         'thank': 2,\n",
       "         'a': 1,\n",
       "         'good': 1,\n",
       "         'guy': 2,\n",
       "         'shut': 1,\n",
       "         'up': 1,\n",
       "         'i': 1,\n",
       "         'am': 1,\n",
       "         'so': 1,\n",
       "         'sorry': 1,\n",
       "         'to': 2,\n",
       "         'hear': 1,\n",
       "         'about': 1,\n",
       "         'that': 1,\n",
       "         'bye': 1,\n",
       "         'see': 1,\n",
       "         'tommerow': 1,\n",
       "         'get': 1,\n",
       "         'out': 1,\n",
       "         'of': 1,\n",
       "         'stupid': 1,\n",
       "         'very': 1,\n",
       "         'much': 1,\n",
       "         'for': 2,\n",
       "         'do': 1,\n",
       "         'us': 1,\n",
       "         '?': 1,\n",
       "         'need': 1,\n",
       "         'youself': 1,\n",
       "         'no': 1,\n",
       "         'god': 1})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset with TabularDataset\n",
    "tv_datafields = [(\"TEXT\", TEXT), (\"LABEL\", LABEL)]\n",
    "train = data.TabularDataset(path='data/train.csv', format='csv', skip_header=True, fields=tv_datafields)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "0\n",
      "<pad>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'\"': 22,\n",
       "         'what': 2,\n",
       "         'the': 1,\n",
       "         'fuck': 2,\n",
       "         'are': 2,\n",
       "         'you': 10,\n",
       "         'doing': 1,\n",
       "         'here': 2,\n",
       "         '!': 3,\n",
       "         'really': 3,\n",
       "         'piss': 1,\n",
       "         'me': 1,\n",
       "         'off': 1,\n",
       "         'thank': 2,\n",
       "         'a': 1,\n",
       "         'good': 1,\n",
       "         'guy': 2,\n",
       "         'shut': 1,\n",
       "         'up': 1,\n",
       "         'i': 1,\n",
       "         'am': 1,\n",
       "         'so': 1,\n",
       "         'sorry': 1,\n",
       "         'to': 2,\n",
       "         'hear': 1,\n",
       "         'about': 1,\n",
       "         'that': 1,\n",
       "         'bye': 1,\n",
       "         'see': 1,\n",
       "         'tommerow': 1,\n",
       "         'get': 1,\n",
       "         'out': 1,\n",
       "         'of': 1,\n",
       "         'stupid': 1,\n",
       "         'very': 1,\n",
       "         'much': 1,\n",
       "         'for': 2,\n",
       "         'do': 1,\n",
       "         'us': 1,\n",
       "         '?': 1,\n",
       "         'need': 1,\n",
       "         'youself': 1,\n",
       "         'no': 1,\n",
       "         'god': 1})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset with Dataset\n",
    "# extract the examples for the Dataset\n",
    "examples = []\n",
    "with open(\"data/train.csv\") as f:\n",
    "    for line in f.readlines():\n",
    "        text, label = line.split(',')\n",
    "        try:\n",
    "            label = int(label)\n",
    "        except:\n",
    "            continue\n",
    "        examples.append(data.Example.fromlist([text, label], fields=[(\"TEXT\", TEXT), (\"LABEL\", LABEL)]))\n",
    "\n",
    "train = data.Dataset(examples, fields=[(\"TEXT\", TEXT), (\"LABEL\", LABEL)])\n",
    "\n",
    "# build the vocab on this field\n",
    "TEXT.build_vocab(train)\n",
    "\n",
    "# build the vocab for the Field using the datasets\n",
    "# vectors need to download, which we use the randomly init\n",
    "TEXT.build_vocab(train)\n",
    "print(len(TEXT.vocab))\n",
    "print(TEXT.vocab.stoi[\"<unk>\"])\n",
    "print(TEXT.vocab.itos[1])\n",
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以手动的构建每一个数据集的 `Example` 之后合并成数据集，正如上面 `Dataset` 所示\n",
    "* `Example.fromlist(data, fields)`:  \n",
    "从 `list` 按照 `Field` 构建对应的 `example`\n",
    "* `Example.fromdict`:  \n",
    "从 `dict` 按照 `Field` 构建对应的 `example`\n",
    "* `fromCSV, fromJSON, fromtree, ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'can', 'tell', 'you', 'a', 'story'] 1\n",
      "['i', 'can', 'tell', 'you', 'a', 'story'] 1\n"
     ]
    }
   ],
   "source": [
    "string = [\"I can tell you a story\", 1]\n",
    "example1 = data.Example.fromlist(string, fields=[(\"TEXT\", TEXT), (\"LABEL\", LABEL)])\n",
    "\n",
    "string = {\"TEXT\": \"I can tell you a story\", \"LABEL\": 1}\n",
    "example2 = data.Example.fromdict(string, fields={'TEXT': (\"TEXT\", TEXT), \"LABEL\": (\"LABEL\", LABEL)})\n",
    "\n",
    "print(example1.TEXT, example1.LABEL)\n",
    "print(example2.TEXT, example2.LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Iterator**\n",
    "构建迭代器\n",
    "* `datasets`:  \n",
    "`tuple` 类型，表示 `train, test, validation` 三种不同的数据集类型，下面实例只是用了 `train`\n",
    "* `sort_key`:  \n",
    "排序的方法，这里一般都是使用 `length` 作为文本的排序手段以便之后的高效 `padding`\n",
    "* `batch_sizes`:  \n",
    "一样，这里是对不同的数据集使用不同的 `batch size` 下面的实例是对 `train` 数据集使用 32 大小的 `batch`\n",
    "* `train`:  \n",
    "是否每一个 `iterator` 表示一个训练集\n",
    "* `repeat`:  \n",
    "是否允许在 `multiple epochs` 的时候重复使用一个 `batch`\n",
    "* `shuffle`:  \n",
    "是否打乱 `examples`\n",
    "* `device`:  \n",
    "使用的设备是什么，默认是在 `cpu` 上，`-1` 是 `cpu`\n",
    "* `sort`:  \n",
    "是否按照 `sort_key` 进行排序\n",
    "\n",
    "![](./torchtext.jpg)\n",
    "\n",
    "最常见的 `Iterator` 就是 `BucketIterator` 因为，在做 `batch` 的 `padding` 的时候我们都知道，如果 `padding token` 越多效率越低，在这里使用 `BucketIterator` 的话在内部会将长度一样的一些 `sentence` 尽量的放入一个 `batch` 中，这样的话可以最小化 `padding token` 的数目，提高训练的效率，参数和 `Iterator` 的参数是一样的，所有的操作都是内部完成的。**如果想要启动这一个特性的话，`sort` 参数必须是 `True`**。但是本质上 `BucketIterator, Iterator`两者没有什么差别，实验效果一样，怀疑是对 `Iterator` 也做了相应的优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of Iterator: \n",
      "tensor([1, 0, 1, 1]) \n",
      " tensor([[ 2,  2,  2,  2],\n",
      "        [13, 29, 21,  5],\n",
      "        [40,  4, 32, 14],\n",
      "        [ 8,  2, 30,  3],\n",
      "        [ 6,  1, 10,  5],\n",
      "        [ 3,  1,  3, 28],\n",
      "        [20,  1, 38, 12],\n",
      "        [10,  1,  9,  8],\n",
      "        [ 4,  1,  2, 45],\n",
      "        [ 2,  1,  1,  2]])\n",
      "['\"', 'what', 'the', 'fuck', 'are', 'you', 'doing', 'here', '!', '\"']\n",
      "tensor([1, 0, 0, 0]) \n",
      " tensor([[ 2,  2,  2,  2],\n",
      "        [ 3, 18, 25, 11],\n",
      "        [ 5, 34, 17,  3],\n",
      "        [33,  3, 36,  3],\n",
      "        [26, 41, 37,  6],\n",
      "        [31,  2, 12, 15],\n",
      "        [ 2,  1, 24, 23],\n",
      "        [ 1,  1, 16,  9],\n",
      "        [ 1,  1, 39,  2],\n",
      "        [ 1,  1,  2,  1]])\n",
      "['\"', 'you', 'really', 'piss', 'me', 'off', '\"', '<pad>', '<pad>', '<pad>']\n",
      "tensor([0, 0, 1]) \n",
      " tensor([[ 2,  2,  2],\n",
      "        [22, 11,  3],\n",
      "        [ 4,  3, 35],\n",
      "        [ 2, 44, 42],\n",
      "        [ 1, 27,  2],\n",
      "        [ 1,  7,  1],\n",
      "        [ 1, 13,  1],\n",
      "        [ 1,  3,  1],\n",
      "        [ 1, 19,  1],\n",
      "        [ 1,  7,  1],\n",
      "        [ 1, 43,  1],\n",
      "        [ 1,  2,  1]])\n",
      "['\"', 'god', '!', '\"', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "Sum of padding token: tensor(30)\n",
      "\n",
      "Examples of BucketIterator: \n",
      "tensor([0, 1, 0, 0]) \n",
      " tensor([[ 2,  2,  2,  2],\n",
      "        [18,  3, 22, 29],\n",
      "        [34, 35,  4,  4],\n",
      "        [ 3, 42,  2,  2],\n",
      "        [41,  2,  1,  1],\n",
      "        [ 2,  1,  1,  1]])\n",
      "['\"', 'bye', 'see', 'you', 'tommerow', '\"']\n",
      "tensor([1, 1, 0, 1]) \n",
      " tensor([[ 2,  2,  2,  2],\n",
      "        [13, 21, 11,  3],\n",
      "        [40, 32,  3,  5],\n",
      "        [ 8, 30,  3, 33],\n",
      "        [ 6, 10,  6, 26],\n",
      "        [ 3,  3, 15, 31],\n",
      "        [20, 38, 23,  2],\n",
      "        [10,  9,  9,  1],\n",
      "        [ 4,  2,  2,  1],\n",
      "        [ 2,  1,  1,  1]])\n",
      "['\"', 'what', 'the', 'fuck', 'are', 'you', 'doing', 'here', '!', '\"']\n",
      "tensor([0, 1, 0]) \n",
      " tensor([[ 2,  2,  2],\n",
      "        [11,  5, 25],\n",
      "        [ 3, 14, 17],\n",
      "        [44,  3, 36],\n",
      "        [27,  5, 37],\n",
      "        [ 7, 28, 12],\n",
      "        [13, 12, 24],\n",
      "        [ 3,  8, 16],\n",
      "        [19, 45, 39],\n",
      "        [ 7,  2,  2],\n",
      "        [43,  1,  1],\n",
      "        [ 2,  1,  1]])\n",
      "['\"', 'thank', 'you', 'very', 'much', 'for', 'what', 'you', 'do', 'for', 'us', '\"']\n",
      "Sum of padding token: tensor(14)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test(train_iter):\n",
    "    # print the examples in train iterations\n",
    "    # 按照上面的图示，每一个元素都是一个 batch，但是因为我们，每一个 batch 使用最长的元素进行 padding\n",
    "    padding_count = 0\n",
    "    padding_token = TEXT.vocab.stoi[\"<pad>\"]\n",
    "    \n",
    "    for batch in train_iter:\n",
    "        print(batch.LABEL, '\\n', batch.TEXT)\n",
    "        # show one result from the examples\n",
    "        example0 = batch.TEXT[:, 0]\n",
    "        print([TEXT.vocab.itos[item] for item in example0])\n",
    "        padding_count += (batch.TEXT == padding_token).sum()\n",
    "    print(\"Sum of padding token:\", padding_count)\n",
    "    print()\n",
    "        \n",
    "train_iter = data.Iterator(train, sort_key=lambda x: len(x.TEXT), batch_size=4, device=-1)\n",
    "print(\"Examples of Iterator: \")\n",
    "test(train_iter)\n",
    "train_iter = data.BucketIterator(train, sort_key=lambda x: len(x.TEXT), batch_size=4, sort=True)\n",
    "print(\"Examples of BucketIterator: \")\n",
    "test(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Examples**\n",
    "show some examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
