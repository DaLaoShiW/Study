{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN & NLP\n",
    "对于该笔记的理论知识内容可以参考 `deeplearning.ai`\n",
    "\n",
    "---\n",
    "\n",
    "1. 我们开始讨论序列模型的内容，这种模型的的强大之处在于我们可以借助他们实现对序列输入内容的分类和标记，生成文本序列或者将一个序列转换成另一个序列,并且在 `NLP` 中的几乎所有的问题都是序列问题，借助与 `RNN` 我们可以方便和完善的处理自然语言处理的问题\n",
    "\n",
    "2. `RNN` 简介\n",
    "    1. 和我们的之前讨论的 `CNN` 处理的静态图像不同的是，我们在自然语言处理的问题中，大部分都是序列建模的任务，在这些问题中，我们的建模的观测顺序也是非常重要的(但是 `CNN` 本身也可以看作是一种特征抽取器的话，也是可应用在部分自然语言处理任务中的)\n",
    "    2. 在 `RNN` 中不仅允许神经元可以和本单元内的网络的上下级神经元连接，还可以允许和不同时间步上的其他的神经元发生联系\n",
    "    3. 循环神经网络可以处理变长的输入向量，对于输入的计算向量的应付更加的自然\n",
    "    4. 在 `RNN` 的当前时间步的计算中，不仅依赖与当前的输入张量数据还依赖于上一个时间步的输入张量，因此序列和序列之前的提供的所有输入都是直接相关的，我们可以将这种工作方式称之为 \"工作记忆\"\n",
    "    5. 带有 `sigmoid` 激活函数的 `RNN` 已经在 `2006` 年被证明是图灵完备的，换句话说，只要合适的训练模型的参数， `RNN` 模型可以模拟所有的计算机程序(算法)，当然是合适的调整模型的权值\n",
    "    6. 模型的参数主要分成两个部分  \n",
    "        在 `RNN` 中的权重是共享的，每一个时间步上的权值共享的，这样可以降低模型的复杂度\n",
    "        * 每一个 `RNN` 单元的网络内的参数 \n",
    "        * 每一个时间步之间的传递参数，**这个参数的输入规模和每一个 `RNNCell` 内的神经元的数目规模是一致的**\n",
    "\n",
    "3. 训练方法 `BPTT`  \n",
    "    `BPTT` 随时间反向传播，将 `RNN` 沿着时间轴打开，可以将梯度下降算法在时间轴上运行，简称随时间反向传播(本质上是从前馈的角度上审视 `RNN` 的计算流程)，如果训练的数据是不等长的也没有关系，因为权重共享的原因，我们可以任意的将 `RNN` 单元展开并且训练的参数在每一个时间步上都是一致的\n",
    "\n",
    "4. `RNN` 的工作模式  \n",
    "    在序列分类和序列生成任务中，我们可以输出或者输入的当数据其实是序列的**信息稠密表示**, 可以理解成是对序列的 `encoder`, 或者是 `decoder` 阶段\n",
    "    * 序列分类 : `many-to-one`\n",
    "    * 序列生成 : `one-to-many` 输入一个标量输出向量，可以将上一个时间步的输出当做下一个时间步的输入(实际上的输出是一个概率分布，我们可以在概率分布上按照概率采样生成当前时间步上的输出，详见 `deeplearning.ai`)\n",
    "    * 序列标注 : `many-to-many` 序列到序列的 **等长映射**\n",
    "    * 序列翻译 : `many-to-many` / **`encoder-decoder`**\n",
    "        在 `encoder-decoder` 中，使用单个 `RNN` 实现是可行的，但是有时候如果输入序列和输出序列的差异比较大的话，我们可以考虑使用两个  `RNN` 一个负责编码一个负责解码，在实际的运行过程中，我们需要使用一个特殊的符号作为输入通知网络合适停止编码并进入解码阶段\n",
    "        \n",
    "5. 梯度消失和梯度爆炸\n",
    "    1. `RNN` 网络的深度和层数非常多，容易导致梯度消失和梯度爆炸，梯度消失比较常见，因为在选用的激活函数中，梯度在很大的区间范围内是很小的，所以梯度消失更常见\n",
    "    2. 因为在 `RNN` 中，相邻的时间步是共享权重的，要么都小于 1,要么都大于 1，比前馈网络更容易的导致梯度消失和梯度爆炸的现象\n",
    "    \n",
    "6. `LSTM`  \n",
    "    `LSTM` 将 `RNN` 稍作改造，在内部添加了少数的记忆单元构成 `LSTM` 单元，记忆许多时间步中的误差的内部状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow-RNN\n",
    "---\n",
    "\n",
    "`RNN` 模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'tensorflow.python.ops.rnn_cell' from '/home/lantian/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "tf.nn.rnn_cell    # this is a model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RNNCell`  \n",
    "它是 `TensorFlow` 中实现 `RNN` 的基本单元，每个 `RNNCell` 都有一个 `call` 方法，使用方式是：  \n",
    "`(output, next_state) = call(input, state)`  \n",
    "1. `step 1`  \n",
    "    假设我们有一个初始状态 `h0`，还有输入 `x1`，调用 `call(x1, h0)` 后就可以得到 `(output1, h1)`\n",
    "    ![RNNCell](./RNNCell.jpg)\n",
    "2. `step 2`  \n",
    "    再调用一次call(x2, h1)就可以得到(output2, h2)\n",
    "    ![RNNCell2](./RNNCell2.jpg)\n",
    "也就是说，每调用一次 `RNNCell` 的 `call` 方法，就相当于在时间上“推进了一步”，这就是 `RNNCell` 的基本功能\n",
    "在代码实现上，`RNNCell` 只是一个抽象类，我们用的时候都是用的它的两个子类 `BasicRNNCell` 和 `BasicLSTMCell` 顾名思义，前者是 `RNN` 的基础类，后者是 `LSTM` 的基础类。\n",
    "\n",
    "---\n",
    "主要的属性,每一个 `RNNCell` 单元的内部参数张量规模\n",
    "  * `state_size`\n",
    "  * `output_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 128\n",
      "(32, 128)\n",
      "(32, 128) (32, 128)\n"
     ]
    }
   ],
   "source": [
    "# BasicRNNCell\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128\n",
    "print(cell.state_size, cell.output_size) # 128, 128, BasicRNNCell 中的状态和输出结果是一模一样的，我们需要对输出做一个线性变换\n",
    "\n",
    "inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size, 100是输入的向量 X 的规模\n",
    "h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size), (32, 128)\n",
    "print(h0.shape)\n",
    "output, h1 = cell.call(inputs, h0) #调用call函数\n",
    "\n",
    "print(output.shape, h1.shape) # (32, 128), (32, 128) -> (32, 128) -> (32, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"mul_2:0\", shape=(32, 128), dtype=float32)\n",
      "Tensor(\"add_1:0\", shape=(32, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# BasicLSTMCell\n",
    "lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)\n",
    "inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\n",
    "h0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态\n",
    "output, h1 = lstm_cell.call(inputs, h0)\n",
    "\n",
    "# LSTMCell 存在两个输出的张量，一种记忆细胞一种输出结果，规模一致的不同的参数\n",
    "print(h1.h)  # shape=(32, 128)\n",
    "print(h1.c)  # shape=(32, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `dynamic_rnn`  \n",
    "    基础的 `RNNCell` 有一个很明显的问题：  \n",
    "      对于单个的 `RNNCell` ，我们使用它的 `call` 函数进行运算时，只是在序列时间上前进了一步,比如使用 `x1、h0` 得到 `h1` ，通过 `x2、h1` 得到 `h2` 等，如果我们的序列长度为 `10`，就要调用 `10` 次 `call` 函数，比较麻烦。对此，`TensorFlow` 提供了一个 `tf.nn.dynamic_rnn` 函数，使用该函数就相当于调用了 `n` 次 `call` 函数，即通过 `{h0,x1, x2, ..., xn}` 直接得 `{h1,h2, ...,hn}`  \n",
    "    **得到的 `outputs` 就是 `time_steps` 步里所有的输出,它的形状为 `(batch_size, time_steps, cell.output_size)`。`state` 是最后一步的隐状态，它的形状为 `(batch_size, cell.state_size)`**\n",
    "    \n",
    "2. `MultiRNNCell` 堆叠 `RNNCell`\n",
    "    有时候\"单层\"(这里的单层不是时间步上的层数)的 `RNN` 并不能起到很好的效果，我们需要将 `RNN` 堆叠几层，通过 `MultiRNNCell` 得到的 `cell` 并不是什么新鲜事物，它实际也是 `RNNCell` 的子类，因此也有 `call` 方法、`state_size` 和 `output_size` 属性。同样可以通过 `tf.nn.dynamic_rnn` 来一次运行多步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128, 128)\n",
      "(<tf.Tensor 'cell_0/cell_0/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>, <tf.Tensor 'cell_1/cell_1/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>, <tf.Tensor 'cell_2/cell_2/basic_rnn_cell/Tanh:0' shape=(32, 128) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "# 每调用一次这个函数就返回一个BasicRNNCell\n",
    "def get_a_cell():\n",
    "    return tf.nn.rnn_cell.BasicRNNCell(num_units=128)\n",
    "\n",
    "# 用tf.nn.rnn_cell MultiRNNCell创建3层RNN\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() for _ in range(3)]) # 3层RNN\n",
    "\n",
    "# 得到的cell实际也是RNNCell的子类\n",
    "# 它的state_size是(128, 128, 128),这是一个时间步上的层的大小规模\n",
    "# (128, 128, 128)并不是128x128x128的意思\n",
    "# 而是表示共有3个隐层状态，每个隐层状态的大小为128\n",
    "print(cell.state_size) # (128, 128, 128)\n",
    "\n",
    "# 使用对应的call函数\n",
    "inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size\n",
    "h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，传递给第一个时间步\n",
    "output, h1 = cell.call(inputs, h0)   # 上一个时间步的状态和当前时间步的输入\n",
    "print(h1) # tuple中含有3个32x128的向量，最后一个时间步3个隐含层全部输出状态,output是当前时间步的结果的输出张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 注意点\n",
    "    1. 查看 `rnn_cell.call` 可以发现在 `BasicRNNCell` 中， `output` 其实和隐状态 `state` 的值是一样的。因此，我们还需要额外对输出定义新的变换，才能得到图中真正的输出 `y`。由于 `output` 和隐状态是一回事，所以在 `BasicRNNCell` 中，`state_size` 永远等于 `output_size` 。`TensorFlow` 是出于尽量精简的目的来定义 `BasicRNNCell` 的，所以省略了输出参数，我们这里一定要弄清楚它和图中原始 `RNN` 定义的联系与区别。如果我们处理的是分类问题，那么我们还需要对 `new_h` 添加单独的 `Softmax` 层才能得到最后的分类概率输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM & GRU\n",
    "---\n",
    "\n",
    "1. `LSTM` 是为了解决 `RNN` 在梯度消失和梯度爆炸的情况下引入的一种学习长时依赖的神经网络，并逐渐成为 `RNN` 事实上的标准\n",
    "2. `LSTM` 将 `RNN` 中的普通的神经元替换成了内部拥有少量记忆结构的 `LSTM` 单元\n",
    "3. **`LSTM` 因为存在有一个线性激活函数和固定权值是1的自连接，保证了内部的局部偏导是1**,在一定程度上减轻了梯度消失的现象\n",
    "4. `LSTM` 本质上是在学习单元内的一些门结构\n",
    "    * 遗忘门 : 内部的循环连接按照比例缩放的遗忘门结构，允许网络，执行遗忘操作，**初始值设定成1保证网络从记忆状态开始工作**，在 `tensorflow` 中对该门的初始化否是 1\n",
    "    * 输出门 : 对输出的值进行缩放\n",
    "    * 更新门 : 对旧的值是否更新的权值设定\n",
    "5. 窥视孔连接 : `tensorflow` 中使用参数 `use_peepholes` 支持使用窥视孔连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量嵌入 `Word2Vec`\n",
    "---\n",
    "\n",
    "1. 词向量是一种第文本数据的有效描述，是在计算机中表示词的有效方式，该方法可以有效的处理大规模的文本语料库\n",
    "2. 原本的词表示系统中\n",
    "    * `One-hot` 独热编码表示词在词汇表中的数据，但是该方法会导致词的表示维度非常高，其次独热编码没有办法表示词之间的相互关系(比如相似性和相异性甚至是词联想的信息)，并且这种关联是存在并且非常有用的\n",
    "    * 依据**共生关系**表示词，在一个大量的文本语料库中，统计一定范围内的词汇利用这些附近的词汇规范化的表示每一个词语，然后使用 `PCA` 等方法对这些向量特征进行降维得到更加稠密的数据表示，但是这样需要学习一个共生矩阵(长宽规模都是词典的长度)\n",
    "    * 利用 `RNN` 的 `skip-gram` 的模型计算词汇的 `word embedding` 的表示方式，学习的是一个**嵌入矩阵**\n",
    "3. 使用无监督的方式创建一种语义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
