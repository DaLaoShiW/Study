{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "---\n",
    "\n",
    "### 数据流图\n",
    "\n",
    "* 数据流图是一种可以用来定义计算结构的特殊的有向图\n",
    "\n",
    "* 数据流图的本质是一组连接在一起的函数(可以定义计算公式，和运算架构)，每一个函数可以将输出传递给多个(0, 1, n)个级联的函数\n",
    "\n",
    "* 组成成分\n",
    "\n",
    "  * 节点（Op） : 代表对数据的运算和某种操作\n",
    "    * `input` 节点 : 传入初始数据\n",
    "    * `output` 节点 : 传出结果数据\n",
    "    * 每一个Op可以接收张量作为参数，将值传递给下一个直接连接的节点(Op)\n",
    "  * 边     : 对节点传入的的数据(有向边)\n",
    "\n",
    "* 注意点\n",
    "\n",
    "  1. 运算顺序 : \n",
    "\n",
    "     实际的运算顺序我们无从知晓，但是我们需要知道只要节点之间的运算是没有相关联的话，我们可以不考虑运算顺序的问题\n",
    "\n",
    "  2. 任何节点都可以将数据传递给数据流图中的任何后继节点，无论两者之间发生了多少计算\n",
    "\n",
    "  3. 定义好我们的数据流图之后,我们可以抽象的理解我们的的数据流图是一个有输入输出的离散构件\n",
    "\n",
    "  4. 节点依赖关系\n",
    "\n",
    "     * 我们利用节点的依赖关系可以有效的对必要的以来节点执行计算，没有必要执行对整个图的计算，这就是依赖关系的用处\n",
    "\n",
    "       背后使用**栈**的方式来管理\n",
    "\n",
    "       * 将所有的以来节点从最终输出节点开始一次入栈可以保证只计算依赖节点，节约计算时间\n",
    "\n",
    "     * 依赖关系具有传递性\n",
    "\n",
    "       * 直接依赖节点\n",
    "       * 间接依赖节点\n",
    "\n",
    "     * 循环依赖\n",
    "\n",
    "       前一个节点依赖于后一个节点的输出的情况，会导致死锁问题(循环依赖的出现)\n",
    "\n",
    "       不支持的原因\n",
    "\n",
    "       1. 程序不能有效的终止\n",
    "       2. 信息追踪不可能\n",
    "       3. 结果上溢或者下溢或者0\n",
    "\n",
    "       数据流图的展开可以实现有限次的循环依赖(模拟有用的循环依赖)\n",
    "\n",
    "### Tensorflow的工作模式\n",
    "\n",
    "* 模式\n",
    "\n",
    "  1. 定义数据流图\n",
    "  2. 运行数据流图\n",
    "\n",
    "* 实例\n",
    "\n",
    "  定义计算公式的数据流图\n",
    "\n",
    "  $$ y = 5 * 3 + (5 + 3)$$\n",
    "\n",
    "* 节点  \n",
    "    * `tf.constant` : 定义常量节点\n",
    "    * `tf.multiply` : 定义乘法节点\n",
    "    * `tf.subtrace` : 定义减法节点\n",
    "    * `tf.negative` : 定义相反数节点\n",
    "    * `tf.div`      : 定义整数除法节点\n",
    "    * `tf.truediv`  : 定义浮点数除法节点\n",
    "    * `tf.mod`      : 定义取模节点\n",
    "    * `tf.square`   : 定义平方计算节点\n",
    "    * `tf.matmul`   : 定义矩阵乘法计算节点\n",
    "* Session  \n",
    "    * `tf.Session.graph` : 引用了数据流图的描述\n",
    "    * `tf.Session.run`          : 对不同的节点执行运算,**返回numpy数组**\n",
    "    * `tf.Session.close` : 关闭会话，释放资源\n",
    "* writer  \n",
    "    * `tf.summary.FileWriter(path, graph)` : 将数据流图的描述写入\n",
    "    * `tensorboard --logdir='my_graph` : 可视化窗口查看数据流图\n",
    "    * `writer.close` : 关闭释放资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据流图的定义,运行不会有任何输出\n",
    "a = tf.constant(5, name = 'input_a')\n",
    "b = tf.constant(3, name = 'input_b')\n",
    "c = tf.multiply(a, b, name = 'mul_c')    # 5 * 3 节点\n",
    "d = tf.add(a, b, name = 'add_d')    # 5 + 3 节点\n",
    "e = tf.add(c, d, name = 'add_e')    # 5 * 3 + (5 + 3) 节点\n",
    "\n",
    "s = tf.square(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# 数据流图的定义\n",
    "sess = tf.Session()\n",
    "print(sess.run(c))    # 运行c节点，逐渐计算依赖关系,可以更改运行成别的节点，得到不一样的输出\n",
    "print(sess.run(s))    # 5 * 5\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 使用tensorboard\n",
    "writer = tf.summary.FileWriter('../../test/book/my_graph', sess.graph)    # 将数据流图的描述放置在目录下\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 张量\n",
    "---\n",
    "1. 张量 : n维矩阵的抽象\n",
    "    * 1D张量 : 向量\n",
    "    * 2D张量 : 2维矩阵\n",
    "    * 张量可以使用 list / tuple 定义形状\n",
    "        ```\n",
    "        shape0 = []        # 定义了0阶张量，标量\n",
    "        shape1 = [3]       # 定义了1阶张量，向量，长度是3\n",
    "        shape2 = [3, 2]    # 定义了2D张量，形状是[3, 2]\n",
    "        shape3 = [None]    # 任意长度的向量\n",
    "        shape4 = None      # 任意规模的张量\n",
    "        ```\n",
    "2. 接收张量的数据流图\n",
    "    * 简化`input`节点\n",
    "    * 直接依赖节点变少\n",
    "    * 灵活性增强，计算速度提升(矩阵运算)\n",
    "3. 实例\n",
    "\n",
    "  定义计算公式的数据流图\n",
    "\n",
    "  $$ y = 5 * 3 + (5 + 3)$$\n",
    "  \n",
    "  * 节点\n",
    "  \n",
    "      * `tf.reduce_prod` : 累乘张量\n",
    "      * `tf.reduce_sum` : 累加张量\n",
    "      * `tf.shape` : 获得张量的形状的Op\n",
    "  * Tensor对象 = 张量\n",
    "\n",
    "4. 数据类型\n",
    "    1. Tensorflow可以接收 **Python数值，bool,str,list**\n",
    "        * 数值 : 0阶张量（标量）\n",
    "        * list(1) : 1阶张量\n",
    "        * list(n) : n阶张量\n",
    "    2. 常用数据类型,**常用数据类型添加在每一个节点函数的最后声明节点的数据类型,`dtype`**\n",
    "        1. tf.string\n",
    "        2. tf.bool\n",
    "        3. tf.float32 / 64\n",
    "        4. tf.int64 / 32 / 16\n",
    "        5. tf.unint8\n",
    "    3. 使用Numpy手动定义Tensor对象\n",
    "        \n",
    "        * 需要注意，tf中的数据类型和numpy中的数据类型中(数值类型和bool类型都是完全一致的，但是string不一致)，我们可以使用numpy定义string数组，但是只要不定义dtype即可传给tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.constant([5, 3], name = 'input_a')    # input节点只有一个张量节点\n",
    "b = tf.reduce_prod(a, name = 'input_b')      # 5 * 3\n",
    "c = tf.reduce_sum(a, name = 'sum_c')         # 5 + 3\n",
    "d = tf.add(c, b, name = 'add_d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True True True\n",
      "Tensor(\"fruit:0\", shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# numpy and tensor\n",
    "print(np.int32 == tf.int32, np.int64 == tf.int64, np.float32 == tf.float32, np.float64 == tf.float64,\\\n",
    "     np.bool == tf.bool)\n",
    "string = np.array(['apple', 'banana', 'pear'])\n",
    "test_op = tf.constant(string, name = 'fruit')\n",
    "print(test_op)    # 返回tensor对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 3, 4]), array([4], dtype=int32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 张量的规模\n",
    "a = tf.constant(np.array([1, 2, 3, 4]), name = 'input_a')\n",
    "b = tf.shape(a)\n",
    "sess.run([a, b])\n",
    "# print(a, b)    # a是一个4长度的向量，b是一个向量代表形状,请使用 sess.run 执行 b 可以查看到结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Operation\n",
    "---\n",
    "1. 节点(Op) : 是对Tensor对象执行运算的节点，计算完毕之后会返回0或者多个张量(**返回值是张量的操作句柄**)\n",
    "2. 节点的分类\n",
    "\n",
    "    * 计算型节点\n",
    "    * 特殊节点 : 无输入无输出，执行特定的功能(初始化等等)\n",
    "3. 属性\n",
    "    \n",
    "    * `name` : 标识节点，在tensorboard中可以查询和方便引用\n",
    "4. 运算符重载 : 无法指定name,需要使用Op制定name\n",
    "    \n",
    "    * `-x`, `tf.negative` : 相反数\n",
    "    * `~x`, `tf.logic_not` : 逻辑非,只适用于bool类型\n",
    "    * `abs(x)`, `tf.abs` : 绝对值\n",
    "    * `x | y` : 逻辑或\n",
    "    * `x ^ y` : 逻辑异或\n",
    "    * `x & y` : 逻辑与\n",
    "    * `tf.pow`\n",
    "    * `< / <= / > / >=`\n",
    "    * `+ | - | * | / | //` : 浮点数除法，向下取整除法\n",
    "    * `tf.equal / tf.not_equal` : 判断张量是否相同,返回numpy的bool类型掩码数组,逐元素判断是否相同\n",
    "    * `==` : 判断两个Tensor对象是不是一个对象，不是判断相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LogicalNot_6:0\", shape=(), dtype=bool)\n",
      "Tensor(\"Neg_6:0\", shape=(), dtype=int64)\n",
      "Tensor(\"Abs_6:0\", shape=(), dtype=int64)\n",
      "[ True False  True]\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# ~x\n",
    "a = tf.constant(np.array(6, dtype = np.bool), name = 'bool')\n",
    "b = ~a\n",
    "print(b)\n",
    "\n",
    "# -x\n",
    "a = tf.constant(np.array(6, dtype = np.int), name = 'int')\n",
    "b = - a\n",
    "print(b)\n",
    "\n",
    "# abs(x)\n",
    "a = tf.constant(np.array(-6, dtype = np.int), name = 'abs')\n",
    "b = abs(a)\n",
    "print(b)\n",
    "\n",
    "# tf.equal\n",
    "a = tf.constant(np.array([1, 2, 3], dtype = np.int), name = 'bool1')\n",
    "b = tf.constant(np.array([1, 3, 3], dtype = np.int), name = 'bool2')\n",
    "c = tf.equal(a, b)\n",
    "\n",
    "sess = tf.Session()\n",
    "ans = sess.run(c)\n",
    "print(ans)\n",
    "\n",
    "# == \n",
    "print(a == a)\n",
    "print(a == b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Graph\n",
    "---\n",
    "1. Graph作用\n",
    "    * 划清图之间的界限\n",
    "    * 让多个图协同工作\n",
    "2. 注意点\n",
    "    * 默认存在一张数据流图Graph对象，在其他图的上下文管理之外的节点会被自动的加入到该图中\n",
    "    * 大多数的程序中，只需要使用默认数据流图即可，除非我们想要进行图之间的协作\n",
    "3. 方法\n",
    "    * `tf.Graph` : 创建新的数据流图,获得句柄\n",
    "    * `tf.Graph.as_default` : 上下文管理器,管理器中加入的节点会被自动的划入图中\n",
    "    * `tf.get_default_graph` : 获得默认数据流图的句柄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    # 加入g数据流图中\n",
    "    a = tf.mul(2, 3)\n",
    "\n",
    "with tf.get_default_graph():\n",
    "    # 加入默认数据流图中\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Session\n",
    "---\n",
    "1. 会话器Session\n",
    "    \n",
    "    * `tf.Session(target, graph, config)`\n",
    "        1. `target`：　和分布式计算有关，这里暂时不提及\n",
    "        2. `graph`：　默认是None(默认数据流图)，如果不是在一个with语句块打开的图上下文管理器中打开Session还是建议传入Graph对象\n",
    "        3. `config`：　设置数据流图的优化参数和日志选项等等\n",
    "    * `tf.Session.run（fetches, feed_dict）`\n",
    "        \n",
    "       使用run方法来计算期望的Tensor对象的输出\n",
    "       * fetches : 接收任意的数据流图的元素(Op, tensor)\n",
    "          \n",
    "          1. fetches 是单个张量\n",
    "          2. fetches 是张量的列表\n",
    "       * feed_dict : 用于覆盖tensor对象，使用字典类型指定覆盖方式，覆盖必须要保证覆盖之后的值可以转变或者就是覆盖前的值的类型\n",
    "           \n",
    "          1. 目的 : 在测试的时候，可以指定输入的测试值，这样可以保证我们的测试计算的速度，可以不用计算替换之前的一些步骤，直接使用替换的值，加快测试的时间\n",
    "    * `tf.Session.close` : 释放资源\n",
    "    * 上下文管理器的会话方式推荐 : session的关闭和开启是自动的\n",
    "     \n",
    "2. 新节点对象Op\n",
    "    \n",
    "    * `tf.global_variables_initializer` : \n",
    "    \n",
    "        准备要使用的所有Variables对象,但是不输出任何值，换言之输出的结果是None,他的目的实质性某一种特殊的任务，是一个特殊的功能性节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 等价构造\n",
    "sess = tf.Session()\n",
    "sess = tf.Session(graph = tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[6, 5]\n"
     ]
    }
   ],
   "source": [
    "# sess.run 计算张量，张量是tf.multiply的输出(输出是张量的句柄),tf便会依次计算b值所有的以来输出节点并顺序执行\n",
    "a = tf.multiply(2, 3)\n",
    "b = tf.add(2, 3)\n",
    "sess = tf.Session()\n",
    "print(sess.run(a))\n",
    "\n",
    "print(sess.run([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "# tf.global_variables_initializer\n",
    "print(type(sess.run(tf.global_variables_initializer())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed_dict\n",
    "a = tf.add(2, 5)\n",
    "b = tf.multiply(a, 3)\n",
    "sess = tf.Session()\n",
    "print(sess.run(b))\n",
    "replace_dict = {a: 15}\n",
    "sess.run(b, feed_dict = replace_dict)    # 使用15替换a中原来的7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# session 上下文管理器\n",
    "with tf.Session() as sess:\n",
    "    # 运行数据流图\n",
    "    pass\n",
    "# sess自动关闭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 占位节点(Op)\n",
    "---\n",
    "1. 我们之前的例子中，所有的输入都是我们指定的constant常量，但是我们更多的时候还是希望从用户那里直接获得输入的值，这时候，需要使用占位节点\n",
    "2. 我们可以利用占位节点创建输入节点\n",
    "3. 使用方式\n",
    "    \n",
    "    * `tf.placeholder(type, shape, name)`\n",
    "        \n",
    "        1. `dtype` : 限制和约定了传入数据的数据类型，必须指定\n",
    "        2. `shape` : 约定了传入 `tensor` 对象的形状，默认是 `None`，可以接收任意形状的张量 `tensor`\n",
    "        3. `name` : 标识符\n",
    "    * `feed_dict + tf.placeholder`\n",
    "    \n",
    "        1. 必须在 `feed_dict` 中为每一个等待计算的占位节点分配一个键值对，只需要我们加入依赖的占位节点就可以，不依赖的可以不用管它(因为懒惰机制不会计算到)\n",
    "        2. 不要试图将 `placeholder` 节点加入 `run` 计算，会引发异常,除非你加入了 `feed_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "Safe !\n"
     ]
    }
   ],
   "source": [
    "# 创建一个长度是2的向量，数据类型是int32\n",
    "a = tf.placeholder(tf.int32, shape = [2], name = 'my_input')\n",
    "\n",
    "b = tf.reduce_prod(a, name = 'prod_b')\n",
    "c = tf.reduce_sum(a, name = 'sum_c')\n",
    "\n",
    "d = tf.add(b, c, name = 'add_d')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 向量必须长度是2, 类型必须是int32, 否则会引发异常\n",
    "    input_dict = {a: np.array([5, 3], dtype = np.int32)}\n",
    "    print(sess.run(d, feed_dict = input_dict))\n",
    "    try:\n",
    "        sess.run(a, feed_dict = {a: np.array([5, 3], dtype = np.int32)})\n",
    "        print(\"Safe !\")\n",
    "    except Exception as e:\n",
    "        print(\"Some error\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Variable对象\n",
    "---\n",
    "1. 在机器学习中，Tensor对象和Op对象都是不可变动的，但是我们需要一部分参数是变动的保证算法的可执行，Variable中可以保存在动词调用中**可变张量值**\n",
    "    \n",
    "    `tf.Variable(data, dtype, name)`\n",
    "    \n",
    "2. 作用机制 : \n",
    "\n",
    "    1. Variable对象可以用在任何会使用Tensor对象的Tensorflow函数或者Op中，将当前值传给使用它的Op\n",
    "    \n",
    "    2. 状态交给tf.Session管理，在使用Variable之前，需要使用Session对Variable对其初始化，保证session开始追踪这些Variable对象的值\n",
    "    \n",
    "    3. 不同的session可以使用相同的Variable,但是保留自己会话的备份，Variable值之间不互相影响\n",
    "    \n",
    "    4. `tf.Variable(trainable)`\n",
    "        \n",
    "        `trainable` 参数可以用来指定我们的optimizer类是否可以修改这些变量来迭代算法，如果只允许手动改动，设定成False\n",
    "           \n",
    "3. Variable对象是张量（一般阶数较高） : \n",
    "    \n",
    "    * 全0\n",
    "    * 全1\n",
    "    * 随机数填充\n",
    "    \n",
    "4. 初始化Variable对象的Op\n",
    "\n",
    "    每一个Op接收一个shape参数创建指定形状的张量,返回Tensor对象，**将其送入Variable当做参数构造**\n",
    "    \n",
    "    * `tf.zeros` : 创建全0的Variable\n",
    "    * `tf.ones` : 创建全1的Vriable\n",
    "    * `tf.random_normal(shape, mean, stddev)` : 均值是mean, 标准差是stddev正态分布的随机张量\n",
    "    * `tf.random_uniform(shape, minval, maxval)` ：　[minval, maxval] 之间均匀分布的随机张量\n",
    "    * `tf.truncated_normal(shape, mean, stddev)` : 类似normal但是所有的值都不会偏离均值超过 2 * stddev \n",
    "    \n",
    "5. Variable对象的修改\n",
    "\n",
    "    * `tf.Variable.assign` : 为Variable赋予新的值，**这是一个Op，需要在run中运行才会生效**\n",
    "    * `tf.Variable.assign_add(number)` : + number\n",
    "    * `tf.Variable.assign_sub(number)` : - number\n",
    "    \n",
    "    * `tf.assign(var_op, new_state)` : 一样的效果\n",
    "       \n",
    "6. 初始化Variable对象 [Op]\n",
    "    \n",
    "    * `tf.global_variables_initializer` : 初始化全部的Variable对象\n",
    "    * `tf.variables_initializer` : 初始化部分的Variable对象(列表)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 8]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable使用方式\n",
    "sess = tf.Session()\n",
    "my_var = tf.Variable(3, name = 'my_var')\n",
    "\n",
    "a = tf.add(5, my_var)\n",
    "mul = tf.multiply(8, my_var)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run([mul, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.22568321,  3.75998259],\n",
       "       [ 4.82036686,  6.12445593]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化Op\n",
    "z = tf.zeros([2, 2])\n",
    "o = tf.ones([6])\n",
    "uniform = tf.random_uniform([3, 3, 3], minval = 0, maxval = 10)    # 形状是 3, 3, 3 在 [0, 10]之间均匀分布的随机数\n",
    "normal = tf.random_normal(shape = [3, 3, 3], mean = 0.0, stddev = 2.0)\n",
    "trunc = tf.truncated_normal([2, 2], mean = 5.0, stddev = 1.0)\n",
    "\n",
    "# 包装成Vriable\n",
    "random = tf.Variable(trunc)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable 初始化\n",
    "# global_variables_initializer : 全部变量初始化\n",
    "# variables_initializer : 部分变量初始化,使用列表指定初始化的Variable的Op句柄\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "init = tf.variables_initializer([random])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Assign_2:0\", shape=(), dtype=int32_ref)\n",
      "2\n",
      "4\n",
      "8\n",
      "9\n",
      "8\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Variable 的重新赋值\n",
    "\n",
    "my_var = tf.Variable(1)\n",
    "# my_var = my_var * 2\n",
    "my_var_mul_2 = my_var.assign(my_var * 2)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)    # 初始化Variable节点,开始跟踪变量Op对象\n",
    "    print(sess.run(my_var_mul_2))    # 1 * 2\n",
    "    print(sess.run(my_var_mul_2))    # 2 * 2\n",
    "    print(sess.run(my_var_mul_2))    # 4 * 2\n",
    "    \n",
    "    print(sess.run(my_var.assign_add(1)))    # 8 + 1\n",
    "    print(sess.run(my_var.assign_sub(1)))    # 9 - 1\n",
    "    print(sess.run(tf.assign(my_var, my_var * 3)))    # my_var = my_var * 3 -> 8 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# session不共享Variable\n",
    "\n",
    "my_var = tf.Variable(0)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "change_2 = my_var.assign_add(10)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(change_2))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # 值不共享\n",
    "    sess.run(init)\n",
    "    print(sess.run(change_2))\n",
    "    sess.run(init)\n",
    "    print(sess.run(my_var))\n",
    "    \n",
    "# trainable\n",
    "my_var = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 名称作用域\n",
    "---\n",
    "1. Tensorflow中为我们提供了一种帮助用户组织数据流图的机制 : 名称作用域，在可视化的时候也非常的有用\n",
    "2. 作用\n",
    "    \n",
    "    * 名称作用域可以将Op划分到更大的，有名称的语句块中(封装)，使得可视化的效果更好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 名称作用域\n",
    "with tf.name_scope(\"scope_a\"):\n",
    "    a = tf.add(1, 2, name='a_add')\n",
    "    b = tf.multiply(a, 3, name='a_mul')\n",
    "    \n",
    "with tf.name_scope(\"scope_b\"):\n",
    "    c = tf.add(4, 5, name='b_add')\n",
    "    d = tf.multiply(c, 6, name='b_mul')\n",
    "\n",
    "e = tf.add(b, d, name='output')\n",
    "\n",
    "# 可视化\n",
    "writer = tf.summary.FileWriter('../../test/book/my_graph/', graph = tf.get_default_graph())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow 基础实践\n",
    "---\n",
    "tensorboard 实践函数\n",
    "  \n",
    "* `tf.summary.scalar(name, tensor)` : 创建汇总数据\n",
    "    \n",
    "    1. name : 在tensorboard中的纵坐标的名称\n",
    "    \n",
    "    2. tensor : 是纵坐标的值\n",
    "    \n",
    "    P.S. : **一种汇总数据对应于一个二维的坐标图，多个对应多个二维坐标图**\n",
    "    \n",
    "    \n",
    "* `tf.summary.merge_all()` : 合并所有的汇总数据到一个Op中，调用一次可以执行所有的汇总，**所有的summary都需要被run才可以放入磁盘**\n",
    "\n",
    "* `write.add_summary(summary, global_step)` : **global_step是迭代的横坐标(x)**,summary是写入的summary数据(y,**汇总数据的运行结果**)，**summary数据写入磁盘**\n",
    "\n",
    "* `writer.flush()` : 刷新写入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # 变量空间，这里的变量是全局的，所以我们分配一个单独的变量空间\n",
    "    with tf.name_scope('variables'):\n",
    "        # 记录运行次数\n",
    "        global_step = tf.Variable(0, dtype = tf.int32, trainable = False, name = 'global_step')\n",
    "        # 记录累和的变化\n",
    "        total_output = tf.Variable(0.0, dtype = tf.float32, trainable = False, name = 'total_output')\n",
    "    \n",
    "    # 变换\n",
    "    with tf.name_scope('transformation'):\n",
    "        # 占位Op\n",
    "        a = tf.placeholder(tf.float32, shape = [None], name = 'input_placeholder_a')\n",
    "        \n",
    "        with tf.name_scope(\"hidden_layer\"):\n",
    "            b = tf.reduce_prod(a, name='product_b')\n",
    "            c = tf.reduce_sum(a, name='sum_c')\n",
    "        \n",
    "        # 输出空间\n",
    "        with tf.name_scope(\"output\"):\n",
    "            output = tf.add(b, c, name='output')\n",
    "        \n",
    "    # 更新变量空间的值\n",
    "    with tf.name_scope('update'):\n",
    "        # 变更的计算\n",
    "        update_total = total_output.assign_add(output)\n",
    "        increment_step = global_step.assign_add(1)\n",
    "        \n",
    "    # tensorboard 汇总数据\n",
    "    with tf.name_scope(\"summaried\"):\n",
    "        avg = tf.div(update_total, tf.cast(increment_step, tf.float32), name = 'average')\n",
    "        # 创建tensorboard汇总数据\n",
    "        tf.summary.scalar('Output', output)\n",
    "        tf.summary.scalar('sum_of_output', update_total)\n",
    "        tf.summary.scalar('avg', avg)\n",
    "    \n",
    "    # 初始化\n",
    "    with tf.name_scope('init'):\n",
    "        init = tf.global_variables_initializer()\n",
    "        # 合并tensorboard的汇总数据到一个Op中\n",
    "        merge_summ = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(graph = graph)\n",
    "writer = tf.summary.FileWriter('../../test/book/my_graph', graph)\n",
    "sess.run(init)\n",
    "\n",
    "def run_graph(input_tensor):\n",
    "    feed_dict = {a: input_tensor}\n",
    "    _, step, summary = sess.run([output, increment_step, merge_summ], feed_dict = feed_dict)\n",
    "    writer.add_summary(summary, global_step=step)\n",
    "\n",
    "run_graph([2, 8])\n",
    "run_graph([3, 1, 3, 3])\n",
    "run_graph([8])\n",
    "run_graph([7, 3, 1])\n",
    "run_graph([6, 3])\n",
    "run_graph([0, 2])\n",
    "run_graph([4, 5, 6])\n",
    "\n",
    "writer.flush()    # 汇总全部写入磁盘\n",
    "writer.close()\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
