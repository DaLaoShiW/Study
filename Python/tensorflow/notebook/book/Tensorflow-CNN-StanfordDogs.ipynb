{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow \n",
    "---\n",
    "\n",
    "1. 目的 : 构建 `CNN` 网络，实现对数据集 `StanfordDogs` 的狗的品种预测，查看预测的准确性\n",
    "2. CNN 网络架构 : 使用 `AlexNet` 实现，随着网络的深度加大，高度和宽度会减小，但是深度(通道)会增加\n",
    "3. 数据集说明\n",
    "    1. 包含有 120 个狗的种类的不同的图片\n",
    "    2. 80% 用作训练， 20% 用作测试\n",
    "    3. 数据集中的图片的**格式不同，尺寸不同**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找打文件的路径\n",
    "path = '/home/lantian/Downloads/nloads/StanfordDog/Images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预处理和TFRecord\n",
    "---\n",
    "1. 图像的尺寸不同，图像的类型不同所以我们需要采用不同的方式预处理成相同类型的图片然后转化成 `TFRecord` 文件中去处理\n",
    "2. 预处理和对转换类型有助于加速训练步骤，但是处理的时间相对来说也会比较长\n",
    "3. 对于 `lazy loading` 导致计算图变得非常巨大的时候，使用 `sess.run` 的速度将会越来越慢，这是因为计算图没有变成只读导致计算图不断的变化变大引起的(计算图变大但是只要不执行 `sess.run` 并没有什么影响，就是内存吃的比较多)，就像下面的例子中我们不能再创建 `TFRecord` 文件的循环中加入 `sess.run` 的函数执行获得张量，相反的，我们可以考虑采用其他的图形处理库 `PIL` 将图片数据解析将数据保存在 `TFRecord` 文件中\n",
    "4. `PIL` 写入 `TFRecord` 文件的注意点\n",
    "   * 写入的时候，直接执行 `Image.tobytes()` 将图片数据直接处理成二进制文件文件(每一个像素数据都是 uint8 类型的)，然后直接写入 `TFReocrd` 文件\n",
    "   * 读取的时候，使用方法 `tf.decode_raw(img_raw, tf.uint8)` 直接读取\n",
    "   * 因为 `PIL` 中的数据存储结构是 `(width, height)` 类型，和我们的写入之前 `reshape` 的结构是刚好相反的(`(height, width)`),这里需要注意反过来对图片张量 `reshape`\n",
    "   * 恢复 `label` 的时候，需要使用 `tf.cast(label, tf.string)` 的方式从张量恢复成二进制字符串，之后再用 `decode` 解码成字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFRecord 文件的读写最佳实践\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 写\n",
    "\n",
    "'''\n",
    "img = Image.open('test.png')\n",
    "img = img.convert('L')\n",
    "img = img.resize((250, 151))\n",
    "img = img.tobytes()\n",
    "\n",
    "writer = tf.python_io.TFRecordWriter('test.tfrecord')\n",
    "\n",
    "label = 'test'.encode('utf8')\n",
    "\n",
    "example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "        'label' : tf.train.Feature(bytes_list = tf.train.BytesList(value=[label])),\n",
    "        'image' : tf.train.Feature(bytes_list = tf.train.BytesList(value=[img]))\n",
    "    }))\n",
    "writer.write(example.SerializeToString())\n",
    "writer.close()\n",
    "'''\n",
    "\n",
    "# 读\n",
    "\n",
    "fq = tf.train.string_input_producer(['test.tfrecord'])\n",
    "\n",
    "reader = tf.TFRecordReader()\n",
    "_, ser = reader.read(fq)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.local_variables_initializer())\n",
    "coord =  tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "\n",
    "features = tf.parse_single_example(ser, features = {\n",
    "    'label' : tf.FixedLenFeature([], tf.string),\n",
    "    'image' : tf.FixedLenFeature([], tf.string)\n",
    "    })\n",
    "\n",
    "img = tf.decode_raw(features['image'], tf.uint8)\n",
    "img = sess.run(img)\n",
    "img = img.reshape((151, 250))\n",
    "img = Image.fromarray(img)\n",
    "img.show()\n",
    "# label 的恢复处理方式\n",
    "label = tf.cast(features['label'], tf.string)\n",
    "print(sess.run(label).decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 收集数据\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# 编写 TFRecord 文件, 一个文件写入 100 个数据文件及其标签，节省I/O读写次数，这样的做法可以提前的确定一个 Batch 但是不能随机的构建 batch\n",
    "def writer_records_file(dataset, path):\n",
    "    '''\n",
    "    1. dataset - defaultdict 的一个键值对\n",
    "    2. path    - 指定的保存路径\n",
    "    '''\n",
    "    \n",
    "    writer = None\n",
    "    current_index = 0\n",
    "    sess = tf.Session()\n",
    "    # sess.graph.finalize()\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    for breed, images_filenames in dataset.items():\n",
    "        # 种类和对应的文件名队列\n",
    "        print(breed, current_index)\n",
    "        for image_filename in images_filenames:\n",
    "            if current_index % 100 == 0:\n",
    "                if current_index != 0 :\n",
    "                    end = time.time()\n",
    "                if writer:\n",
    "                    # 存在读写器关闭\n",
    "                    writer.close()\n",
    "                \n",
    "                # 不存在读写器创建\n",
    "                record_filename = '%s/%s.tfrecords' % (path, str(current_index))\n",
    "                print(record_filename)\n",
    "                writer = tf.python_io.TFRecordWriter(record_filename)\n",
    "            current_index += 1    \n",
    "            time_photo = time.time()\n",
    "            image_file = tf.read_file(image_filename)\n",
    "            \n",
    "            image_file = Image.open(image_filename)\n",
    "            image_file = image_file.convert('L')\n",
    "            image_file = image_file.resize((250, 151))\n",
    "            img_data = image_file.tobytes()\n",
    "            '''\n",
    "            # 不是 jpeg 文件可以忽略不处理\n",
    "            try:\n",
    "                image = tf.image.decode_jpeg(image_file)\n",
    "            except:\n",
    "                print(image_filename)\n",
    "                continue\n",
    "            \n",
    "            # 转换成灰度图片，加快处理速度\n",
    "            # gray_image = tf.image.rgb_to_grayscale(image)\n",
    "            # resized_gray_image = tf.image.resize_images(gray_image, [250, 151], method = 0)\n",
    "            # print(sess.run(resized_gray_image).shape)\n",
    "            # 转换成整数之后转变成字节数组\n",
    "            pp = tf.cast(resized_gray_image, tf.uint8)\n",
    "            pause = sess.run(pp)\n",
    "            '''\n",
    "            # the default graph become bigger and bigger, lazy loading problem find in CS20, The trap of lazy loading\n",
    "            # 使用PIL读取图片送入计算图中\n",
    "            # image_bytes = pause.tobytes()\n",
    "            \n",
    "            image_label = breed.encode('utf8')\n",
    "            example = tf.train.Example(features = tf.train.Features(feature = {\n",
    "                'label' : tf.train.Feature(bytes_list = tf.train.BytesList(value=[image_label])),\n",
    "                'image' : tf.train.Feature(bytes_list = tf.train.BytesList(value=[img_data]))\n",
    "            }))\n",
    "            \n",
    "            writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "# 加载 TFRecord 文件\n",
    "def read_tfrecord(path):    \n",
    "    sess = tf.Session()\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(glob.glob(path + 'testing-image/*'))\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, ser = reader.read(filename_queue)\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess = sess, coord = coord)\n",
    "    \n",
    "    features = tf.parse_single_example(ser, features = {\n",
    "        'label' : tf.FixedLenFeature([], tf.string),\n",
    "        'image' : tf.FixedLenFeature([], tf.string)\n",
    "    })\n",
    "    \n",
    "    record_image = tf.decode_raw(features['image'], tf.uint8) \n",
    "    # 灰度图像\n",
    "    # image = tf.reshape(record_image, [250, 151, 1])\n",
    "\n",
    "    p = sess.run(record_image)\n",
    "    image = p.reshape((151, 250))\n",
    "    label = tf.cast(features['label'], tf.string)\n",
    "    \n",
    "    # create the shuffle_batch\n",
    "    min_after_dequeue = 10\n",
    "    batch_size = 3\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    # 构建样本队列\n",
    "\n",
    "    image_batch, label_batch = tf.train.shuffle_batch([image, label], batch_size = batch_size, capacity=capacity, min_after_dequeue=min_after_dequeue)\n",
    "    print('before ... ')\n",
    "    return image_batch, label_batch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 找打文件的路径\n",
    "    path = '/home/lantian/Downloads/StanfordDog/Images/'\n",
    "\n",
    "    # get the label and the image\n",
    "    coord = tf.train.Coordinator()\n",
    "    try:\n",
    "        image_batch, label_batch = read_tfrecord(path)\n",
    "        sess = tf.Session()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
    "        img, lab = sess.run([image_batch, label_batch])\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Over the reading ... ')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "\n",
    "    '''\n",
    "    # 提取所有的图片\n",
    "    test_number = 0\n",
    "    train_number = 0\n",
    "    image_filename = glob.glob(path + 'n02*/*.jpg')\n",
    "    training = defaultdict(list)\n",
    "    testing = defaultdict(list)\n",
    "    # filename.split('/')[-2] 抽取的是狗的种类\n",
    "    image_filename_with_breed = map(lambda filename: (filename.split('/')[-2], filename), image_filename)\n",
    "    \n",
    "    for dog_breed, breed_images in groupby(image_filename_with_breed, lambda x : x[0]):\n",
    "        \n",
    "        # 20 % 的数据加入训练集\n",
    "        for i, breed_image in enumerate(breed_images):\n",
    "            if i % 5 == 0:\n",
    "                testing[dog_breed].append(breed_image[1])\n",
    "                test_number += 1\n",
    "            else:\n",
    "                training[dog_breed].append(breed_image[1])\n",
    "                train_number += 1\n",
    "            \n",
    "        # 确保测试集的数目 > 18 % \n",
    "        breed_training_count = len(training[dog_breed])\n",
    "        breed_testing_count = len(testing[dog_breed])\n",
    "        \n",
    "        assert round(breed_testing_count / breed_training_count, 2) > 0.18, \"Do not have the enough test dataset !\"\n",
    "\n",
    "    writer_records_file(testing, path + 'testing-image')\n",
    "    print('testing is over!')\n",
    "    writer_records_file(training, path + 'training-image')\n",
    "    print('trainging is over!')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model 模型构建\n",
    "batch_size = 3\n",
    "\n",
    "# 转换成浮点类型 [0, 1] 更适合卷积处理\n",
    "float_image_batch = tf.image.convert_image_dtype(image_batch, tf.float32)\n",
    "\n",
    "# 第一层卷积和池化\n",
    "conv2d_layer_1 = tf.contrib.layers.convolution2d(float_image_batch, num_outputs=32, kernel_size=(5, 5),\n",
    "                                                activation_fn = tf.nn.relu, weight_initializer = tf.random_normal,\n",
    "                                                stride = (2, 2), trainable= True)\n",
    "pool_layer_1 = tf.nn.max_pool(conv2d_layer_1, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "# 第二层卷积和池化\n",
    "conv2d_layer_2 = tf.contrib.layers.convolution2d(pool_layer_1, num_outputs=64, kernel_size=(5, 5),\n",
    "                                                activation_fn = tf.nn.relu, weight_initializer = tf.random_normal,\n",
    "                                                stride = (1, 1), trainable= True)\n",
    "pool_layer_2 = tf.nn.max_pool(conv2d_layer_1, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "# 全连接\n",
    "flattern_layer_2 = tf.reshape(pool_layer_2, [batch_size, -1])\n",
    "hidden_layer_3 = tf.nn.dropout(tf.contrib.layers.fully_connected(flattern_layer_2, 512), 0.2)\n",
    "\n",
    "# 120 个狗的种类\n",
    "final_layer_4 = tf.contrib.layers.fully_connected(hidden_layer_3, 120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
