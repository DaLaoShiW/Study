{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Operator\n",
    "---\n",
    "\n",
    "#### A\n",
    "---\n",
    "1. `tf.add_n(inputs, name=None)`: 按照元素累加输入向量,但是\n",
    "\n",
    "#### B\n",
    "---\n",
    "\n",
    "#### C\n",
    "---\n",
    "1. `tf.nn.conv2d_transpose` : 反卷积\n",
    "2. `tf.contrib.layers.convolution2d` : 对卷积层的高度封装\n",
    "3. `tf.contrib.layers.fully_connected` : 全连接层\n",
    "4. `tf.contrib.layers.dropout` : 反向随机失活添加\n",
    "5. `tf.contrib.learn.ModeKeys.TRAIN` : 表示状态是训练状态的字符串 `train`\n",
    "6. `tf.cast(x, dtype, name)` : 将 `x` 张量的数据类型转换成 `dtype`\n",
    "\n",
    "#### D\n",
    "---\n",
    "1. `tf.decode_csv` : 一系列\n",
    "2. `tf.decode_raw(bytes, dtype)` : 读取字节数组并转化成制定类型的张量然后返回\n",
    "3. `tf.nn.depthwise_conv2d` : \n",
    "    1. Inception 架构的网络构建\n",
    "    2. 涉及到空洞卷积等新概念 [参考链接](http://blog.csdn.net/mao_xiao_feng/article/details/78003476)\n",
    "4. `tf.layers.dense`: 全连接层高级定义\n",
    "    ```python\n",
    "    tf.layers.dense(inputs, units, activation=None)\n",
    "    ```\n",
    "    * inputs: 输入张量\n",
    "    * units: 输出层神经元数目\n",
    "    * activation: 激活函数，`tf.nn.softmax, tf.nn.relu`\n",
    "\n",
    "#### E\n",
    "---\n",
    "1. `tf.expand_dims(input, axis)` : 张量扩展一个维度\n",
    "    `axis` : 0 ~ N , N 是张量的维度的数目\n",
    "    ```python\n",
    "    tf.expand_dims(input, 0)    # 将一个维度扩展到第一个维度之前\n",
    "    tf.expand_dims(input, -1)    # 将一个维度扩展到最后一个维度\n",
    "    ```\n",
    "    \n",
    "\n",
    "#### F\n",
    "---\n",
    "\n",
    "#### G\n",
    "---\n",
    "1. `tf.group`\n",
    "2. `tf.gradients`\n",
    "3. `Op.get_shape` : 获取张量的规模,结果比 `tf.shape` 更精细\n",
    "4. `graph.finalize()` : 冻结计算图，使用方式同 `sess.graph.finalize()` ,但是更加的灵活，只读化计算图\n",
    "5. `graph.get_operations()` : \n",
    "    \n",
    "    获取当前的计算图的计算节点的列表， 对返回列表`clear / insert / delete` 操作不会成功，但是 `append` 操作会修改计算图，利用该方法可以有效的查看到计算图的规模\n",
    "\n",
    "#### H\n",
    "---\n",
    "\n",
    "#### I\n",
    "---\n",
    "1. `tf.image.convert_image_dtype` :  \n",
    "    图片的类型转换，方便TF处理,图像数据如果使用浮点数表示取值范围在 `[0, 1]`,否则正常使用 `tf.uint8` 的数据类型表示，数据取值范围是 `[0, 255]`\n",
    "2. `tf.image.decode_jpeg` : 解码图像文件成张量\n",
    "3. `tf.image.rgb_to_grayscale(image)` : 转换成灰度图, `image` 是一个图片的3D张量，转换后的通道是1,**所有的颜色值取平均得到灰度的通道值**\n",
    "4. `tf.image.rgb_to_hsv(image)` : 转换成 HSV 空间值\n",
    "5. `tf.image.grayscale_to_rgb(image)` : 没有多大的意义，RGB 通道的值都用灰度值填充\n",
    "6. `tf.image.hsv_to_rgb(image)`\n",
    "7. `tf.image.resize_images(images, size, method=0, align_corners=False)` : \n",
    "    1. 参考连接\n",
    "        * [consult_1](http://blog.csdn.net/zsean/article/details/76383100)\n",
    "        * [consult_2](http://blog.csdn.net/UESTC_C2_403/article/details/72699260)\n",
    "    2. 调整图片的大小到一个制定的标准上，这样做会导致图片可能出现扭曲，一般需要考虑使用裁剪等其他的办法避免扭曲的情况，但是这个方法也常用\n",
    "    调整方法,**返回值也有可能会变成 `tf.float32` 类型的数据**\n",
    "    3. 调整方式\n",
    "       图像大小的调整方式：在Tensorflow中通过tf.image.resize_images函数实现；\n",
    "       * 双线性插值算法（Bilinear interpolation）;Method取值为：0 - 默认\n",
    "       * 最近邻居法（Nearest  neighbor interpolation);Method取值为：1；\n",
    "       * 双三次插值法（Bicubic interpolation);Method取值为：2；\n",
    "       * 面积插值法（Area interpolation) ;Method取值为：3；\n",
    "    4. `e.g.`\n",
    "        ```python\n",
    "        tf.image.resize_images(images, [250, 151], method=0)\n",
    "        ```\n",
    "\n",
    "#### J\n",
    "---\n",
    "\n",
    "#### K\n",
    "---\n",
    "\n",
    "#### L\n",
    "---\n",
    "1. `tf.local_response_normalization` : 构建局部响应归一化\n",
    "2. `tf.losses.mean_squared_error(labels, prediction)`: 均方误差损失函数\n",
    "    * labels: 网络生成输出\n",
    "    * predictions: 实际标签\n",
    "    \n",
    "#### M\n",
    "---\n",
    "1. `tf.minimum(x, y, name)` : 返回相对小一点的张量,并且张量 `x,y` 的张量规模必须一致\n",
    "2. `tf.map_fn(function, input_tensor, dtype)` : 对输入的张量 `input_tensor` 都执行一遍 `function` 函数的结果\n",
    "    * `function` : 处理的函数，返回值由 `dtype` 决定\n",
    "    * `input_tensor` : 制定输入的张量，必须是张量\n",
    "    * 返回的节点执行后产生的是所有的执行的返回结果，和 `Python` 中的高阶函数 `map` 的作用效果基本一致\n",
    "\n",
    "#### N\n",
    "---\n",
    "1. `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "2. `tf.no_op`\n",
    "3. `tf.nn.sigmoid_cross_entroy_with_logits`\n",
    "4. `tf.nn.rnn_cell.BasicRNNCell.call(self, inputs, state)`  \n",
    "    大多数的基础的 `RNN: output = new_state = act(W * input + U * state + B).`\n",
    "5. `tf.nn.rnn_cell.BasicRNNCell(num_units, activation=None, reuse=None)`\n",
    "    基础的 `RNNCell` 类\n",
    "      * `num_units` : `RNNCell` 单元中的神经元数目\n",
    "      * `activation` : 非线性激活函数，常用 `tanh`,内部默认的激活函数是 `tanh`\n",
    "6. `tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None\n",
    ", parallel_iterations=None, swap_memory=False, time_major=False, scope=None)`  \n",
    "    利用循环神经网络单元的 `call` 函数动态的执行多次 `RNNCell` 将网络的计算推进多次\n",
    "    参数\n",
    "      * `cell` : `RNNCell` 的一个实例，一个循环神经网络的一个计算单元，然后展开进行动态计算\n",
    "      * `time_major` : 输入输出张量的规模格式\n",
    "          * `True` : 张量的格式必须限定成 `[max_time, batch_size, depth]`,相对来说计算会更高效\n",
    "          * `False` : 张量的格式必须限定成是 `[batch_size, max_time, depth]`, 因为大多数的机器学中，都会将输入的张量的 `batch_size` 当做是第一维度，所以这里的 `False` 是**默认格式**\n",
    "      * `inputs` :  \n",
    "          循环神经网络的输入，输入的张量的形式和参数 `time_major` 存在关系，见上一条  \n",
    "          加入输入的参数的形式是 `[batch_size, max_time, depth]`, 在这种情况下 `batch_size` 就不用过多的讨论，之后的 `[max_time, depth]` 的含义在于，每一个输入的样本都存在一个最大时间步长(当前的计算的循环神经网络的深度), `depth` 代表的是每一个输入的时间步中的向量的维度\n",
    "      * `initial_state` : `RNN` 的初始化状态,一般取零矩阵，**这里的初始状态是我们网络的第一个时间步接收到的状态张量**\n",
    "      * `dtype` : 运算中的过程类型\n",
    "    输入张量的形式也和 `time_major`有关，但是基本上都是 `(outputs, state)`\n",
    "    \n",
    "    ```python\n",
    "    # create a BasicRNNCell\n",
    "    rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)\n",
    "    \n",
    "    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "    \n",
    "    # defining initial state\n",
    "    initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "    \n",
    "    # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,\n",
    "                                   initial_state=initial_state,\n",
    "                                   dtype=tf.float32)\n",
    "    ```\n",
    "    \n",
    "    ---\n",
    "\n",
    "    ```python\n",
    "    # create 2 LSTMCells\n",
    "    rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]\n",
    "    \n",
    "    # create a RNN cell composed sequentially of a number of RNNCells\n",
    "    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "    \n",
    "    # 'outputs' is a tensor of shape [batch_size, max_time, 256]\n",
    "    # 'state' is a N-tuple where N is the number of LSTMCells containing a\n",
    "    # tf.contrib.rnn.LSTMStateTuple for each cell\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                   inputs=data,\n",
    "                                   dtype=tf.float32)\n",
    "    ```\n",
    "7. `tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)`\n",
    "    `RNNCell` 单元堆叠成一个多层的 `RNN`\n",
    "      * `cells`: 按序构成的 `RNNCell` 的列表\n",
    "      * `state_is_tuple` : \n",
    "        * `If True` , 返回的是一个包含每一层输出状态的元组\n",
    "        * `IF False`, 该行为即将淘汰\n",
    "        \n",
    "8. `tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)`  \n",
    "    在输入的张量 `params` 上抽取出对应的嵌入向量，返回和输入的张量 `params` 数据类型一样的新的张量\n",
    "    1. `params` : 巨大的嵌入矩阵张量\n",
    "    2. `ids` : 从嵌入矩阵 `params` 中得到的嵌入向量，可以是列表,`ids` 可以是高维的列表或者是张量\n",
    "    3. `name`\n",
    "    4. `max_norm` : 可以允许对输出的嵌入向量构建 `L2` 归一化\n",
    "9. `tf.nn.sampled_softmax_loss`\n",
    "10. `tf.nn.l2_loss`: l2正则化损失项，`output = sum(t ** 2) / 2`\n",
    "      \n",
    "#### O\n",
    "---\n",
    "\n",
    "#### P\n",
    "---\n",
    "1. `tf.pack`\n",
    "2. `tf.parse_single_example` : 解析一个 TFRecord 文件生成对应的内容读取对象\n",
    "\n",
    "#### Q\n",
    "---\n",
    "\n",
    "#### R\n",
    "---\n",
    "1. `tf.Runoptions`\n",
    "2. `tf.reshape`\n",
    "3. `tf.range(first, end, stop)` : 和Python-numpy中的 `arange` 使用方式相同\n",
    "4. `tf.read_file(filename, name)` : \n",
    "    从路径中读取文件内容，是二进制字符串，针对不同的文件可能需要解码，如果文件是图像的话，可能会需要使用 `tf.image.decode_raw` 来解开实现对图像像素的读取\n",
    "    ```python\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.decode_jpeg(tf.read_file(photo)))\n",
    "    ```\n",
    "\n",
    "#### S\n",
    "---\n",
    "1. `tf.square`\n",
    "2. `tf.squared_difference(x, y, name)`  \n",
    "    返回 `(x-y)^2` 不过是元素级别的，要求 `y,x` 具有相同的张量规模，返回的张量规模也是一致的\n",
    "3. `tf.sigmoid`\n",
    "4. `tf.shuffle_batch`\n",
    "5. `tf.shape` : 计算张量的形状\n",
    "    \n",
    "    ```python\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.shape([[1, 2, 3], [2, 3, 1]]))\n",
    "    # output\n",
    "    # array([2, 3], dtype=int32)\n",
    "    ```\n",
    "6. `tf.slice(input_, begin, size)` : 从张量中抽取张量\n",
    "    1. `input_` : 代表输入要抽取的张量\n",
    "    2. `begin` : 是一个列表，列表的维度是张量 `input_` 的维度，代表在该维度上起始的 `index` 索引\n",
    "    3. `size` : 是一个列表，列表的维度是张量 `input_` 的维度，代表在该维度上 `begin` 开始之后的连续索引的长度\n",
    "    \n",
    "    ```python\n",
    "    t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                 [[3, 3, 3], [4, 4, 4]],\n",
    "                 [[5, 5, 5], [6, 6, 6]]])\n",
    "    # [1, 0, 0]的1 代表从 [[3, 3, 3], [4, 4, ,4]] 开始索引,索引长度是1\n",
    "    tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]\n",
    "    tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],\n",
    "                                       #   [4, 4, 4]]]\n",
    "    tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],\n",
    "                                       #  [[5, 5, 5]]]\n",
    "    ```\n",
    "7. `sess.graph.finalize()` :   \n",
    "    冻结计算图，保证计算图在之后的过程中不会发生变化，从而避免了 `lazy loading` 中计算图不断修改导致运行越来越慢的错误，之后如果在绘画中制定的计算图中增加或者删除节点的话，都会产生错误表示计算图的变更，**从而保证模型的不变性**\n",
    "\n",
    "#### T\n",
    "---\n",
    "1. `tf.to_float` : 一系列, `tf.cast`\n",
    "2. `tf.TextLineReader`\n",
    "3. `tf.transpose` : 转置\n",
    "4. `tf.train.match_filenames_once` :  \n",
    "    函数来产生文件名列表,将结果送入到 `tf.string_input_producer` 中，产生文件名队列，其实完全可以使用文件名列表代替\n",
    "5. `tf.train.Example` : \n",
    "6. `tf.train.Features` : \n",
    "7. `tf.train.BytesList` :\n",
    "8. `tf.trainable_variables`: 返回所有可以训练的变量 (`trainable = True`)\n",
    "\n",
    "#### U\n",
    "---\n",
    "\n",
    "#### V\n",
    "---\n",
    "\n",
    "#### W\n",
    "---\n",
    "1. `tf.where(input, x=None, y=None, name)` : 返回 `bool` 张量中 `True` 的所在坐标\n",
    "   * 如果 `x,y` 都是 `None` : \n",
    "       返回一个二维的张量，第一维是输入的含有 `True` 的个数，第二个维度是坐标\n",
    "       ```python\n",
    "       sess.run(tf.where([True, False, True]))\n",
    "       # output\n",
    "       # array([[0],\n",
    "       # [2]])\n",
    "       ```\n",
    "   * 如果 `x,y` 都不是 `None` : \n",
    "       1. `x,y` 是相同的规模的张量\n",
    "       2. 作用 : 将 `x` 张量中位置在 `input bool` 数组中为 `True` 的元素不变，其他的替换成对应的在 `y` 中的张量元素\n",
    "       3. [参考连接](http://blog.csdn.net/a_a_ron/article/details/79048446)\n",
    "       4. `e.g.`\n",
    "           ```python\n",
    "           a = np.array([[1,0,0],[0,1,1]])\n",
    "           a1 = np.array([[3,2,3],[4,5,6]])\n",
    "           mask = sess.run(tf.equal(a, 1))    # boardcast 方式扩展1构成一个布尔数组\n",
    "           print(sess.run(tf.where(tf.equal(a,1),a1,1-a1)))\n",
    "           ```\n",
    "\n",
    "#### X\n",
    "---\n",
    "\n",
    "#### Y\n",
    "---\n",
    "\n",
    "#### Z\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-ab16d9c81b89>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-ab16d9c81b89>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    1. `tf.decode_csv` : 一系列\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#### A\n",
    "\n",
    "#### B\n",
    "\n",
    "#### C\n",
    "\n",
    "#### D\n",
    "1. `tf.decode_csv` : 一系列\n",
    "\n",
    "#### E\n",
    "\n",
    "#### F\n",
    "\n",
    "#### G\n",
    "1. `tf.group`\n",
    "2. `tf.gradients`\n",
    "\n",
    "#### H\n",
    "\n",
    "#### I\n",
    "\n",
    "#### J\n",
    "\n",
    "#### K\n",
    "\n",
    "#### L\n",
    "\n",
    "#### M\n",
    "\n",
    "#### N\n",
    "1. `tf.nn.sparse_softmax_cross_entropy_with_logits`\n",
    "2. `tf.no_op`\n",
    "3. `tf.nn.sigmoid_cross_entroy_with_logits`\n",
    "\n",
    "#### O\n",
    "\n",
    "#### P\n",
    "1. `tf.pack`\n",
    "\n",
    "#### Q\n",
    "\n",
    "#### R\n",
    "1. `tf.Runoptions`\n",
    "2. `tf.reshape`\n",
    "\n",
    "#### S\n",
    "1. `tf.square`\n",
    "2. `tf.squared_difference`\n",
    "3. `tf.sigmoid`\n",
    "4. `tf.shuffle_batch`\n",
    "\n",
    "#### T\n",
    "1. `tf.to_float` : 一系列, `tf.cast`\n",
    "2. `tf.TextLineReader`\n",
    "3. `tf.transpose` : 转置\n",
    "\n",
    "#### U\n",
    "\n",
    "#### V\n",
    "\n",
    "#### W\n",
    "\n",
    "#### X\n",
    "\n",
    "#### Y\n",
    "\n",
    "#### Z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
