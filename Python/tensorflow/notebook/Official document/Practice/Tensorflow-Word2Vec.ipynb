{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow-Word2Vec\n",
    "---\n",
    "\n",
    "`Word2Vec` 是 `Google` 团队为自然语言处理任务开发的一种对词进行嵌入获得嵌入矩阵算法，该算法通过对包含有嵌入矩阵的神经网络进行学习得到副产物嵌入矩阵，可以有效的反映出词的特征\n",
    "具体参考\n",
    "  * `deeplearning.ai`\n",
    "  * `Tensorflow Document`\n",
    "  \n",
    "部分资料关于 `Word Embedding`\n",
    "1. 将现实世界的高维度信息抽象的编码成高维度的向量是不明智的选择，实际上，只需要少量的特征就可以实现对于实体的描述，因此对高维度的信息进行其纳入是十分有必要的\n",
    "2. 实现这种 `Word Embeddin` 的方法一般都是基于计数或者是概率化的预测实现的，基于计数的方法计算某词汇与其邻近词汇在一个大型语料库中共同出现的频率及其他统计量，然后将这些统计量映射到一个小型且稠密的向量中。预测方法则试图直接从某词汇的邻近词汇对其进行预测，在此过程中利用已经学习到的小型且稠密的嵌套向量。\n",
    "3. `Word2vec` 是一种可以进行高效率词嵌套学习的预测模型。其两种变体分别为  \n",
    "    相对来说，使用 `Skip-Gram` 的方式去学习在大型数据集中更有效\n",
    "    * 连续词袋模型 `（CBOW）`\n",
    "    * `Skip-Gram` 模型\n",
    "4. 神经概率化的优化方法中，通常已知的一个的上下文单词和一个目标单词，计算目标单词出现在上下文单词附近的可能性，使用 `softmax` 进行计算，但是在大语料库下，计算 `softmax` 的方式非常消耗计算资源，所以可以采用**负采样**的方式实现对 `softmax` 到 `logistic` 的过渡处理，这种过渡的处理方式通过在一个正确采样中虚构很多的错误的负样本实现网络对正样本的学习来达到这个目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCE 噪声对比估计\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Skip-Gram 模型\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
