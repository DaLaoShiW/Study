{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow MNIST CNN\n",
    "建议在了解了 `deeplearning.ai` 中的有关概念之后，再来实现有关的细节\n",
    "\n",
    "使用多层卷积神经网络来优化上一节中对 `Softmax` 的局限性(91.8% 的正确率)\n",
    "\n",
    "---\n",
    "\n",
    "1. CNN 层级单元\n",
    "    \n",
    "    1. 卷积层\n",
    "    2. 线性整流层 : ReLU 函数\n",
    "    3. 池化层\n",
    "    4. 全连接层\n",
    "    \n",
    "    运行速度过慢，直接展示图片，5000次迭代训练的结果\n",
    "    \n",
    "    ![](./result.png)\n",
    "    \n",
    "2. 新引入的 `tensorflow` 函数\n",
    "\n",
    "    1. `tf.nn.conv2d`\n",
    "    2. `tf.nn.softmax`\n",
    "    3. `tf.reshape`\n",
    "    4. ...\n",
    "\n",
    "3. 疑问\n",
    "    \n",
    "    1. 正确率在训练的过程中出现波动的原因 ?\n",
    "        梯度下降的正常现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-e87ff70f2dbb>:32: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/lantian/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/lantian/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lantian/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lantian/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/lantian/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "step 0, training accuracy 0.16\n",
      "step 100, training accuracy 0.72\n",
      "step 200, training accuracy 0.92\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 0.92\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.88\n",
      "step 1100, training accuracy 0.92\n",
      "step 1200, training accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "# ---------- 数据输入 ---------- #\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Functions for downloading and reading MNIST data.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import numpy\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "\n",
    "dataset = read_data_sets('./MNIST_data', one_hot=True)\n",
    "\n",
    "# --------- 定义输入占位节点 ---------- #\n",
    "x = tf.placeholder(\"float\", shape=[None, 784])\n",
    "y_ = tf.placeholder(\"float\", shape=[None, 10])\n",
    "\n",
    "# --------- 定义参数(权重， 偏置) ---------- #\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)    # 保证正态分布的区间维持在 -0.2 ~ 0.2 之间\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积层和池化层\n",
    "# 池化层 : 2 * 2 Max Pooling\n",
    "# 卷积层函数 : tf.nn.conv2d\n",
    "def conv2d(x, W):\n",
    "    # x是输入的tensor,四个维度[batch, height, width, channels]\n",
    "    # W是卷积核tensor,四个维度[height, width, in_channels, out_channels],out_channels可以理解成是过滤器的个数\n",
    "    # strides是对输入四个维度的滑动窗口的步长\n",
    "    # padding='SAME'表示卷积层之后输出和输入的规模不变\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # x是输入的四维tensor\n",
    "    # ksize代表窗口的大小\n",
    "    # strides是步长\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# ---------- 第一卷积层 ---------- #\n",
    "# 第一卷积层的权重，初始化卷积核和偏置参数(32个卷积核，每个卷积核的大小是5 * 5, 因为是灰度图像，没有颜色通道，所以in_channels是1)\n",
    "# 第一层的卷积层的偏置有32个（针对32个卷积核）\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# 重组输入的图片的tensor规模，28,28是图像的高和宽，1是通道数(灰度图片只有一个通道),-1自调整batch的大小\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# 线性整流和池化\n",
    "# 28 - 1 + 1 = 28 -> 28 * 28\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# ( 28 - 2 ) / 2 + 1 = 14 -> 14 * 14\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# ----------- 第二卷积层 ---------- #\n",
    "# 特征数目扩展到64,64个卷积核\n",
    "# h_pool1 作为输入\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# 14 - 1 + 1 = 14 -> 14 * 14\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "# ( 14 - 2 ) / 2 + 1 = 7 -> 7 * 7\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# ---------- 全连接层 ---------- #\n",
    "# 此时的图像的大小是 7 * 7 * 64 (7 * 7 大小，６４个特征)\n",
    "# 3136 -> 1024 的一个全连接层\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# 展开成一维向量 (1, 7 * 7 * 64)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# ---------- 避免过拟合 dropout ---------- #\n",
    "# 神经元被drop的概率\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# --------- Softmax 输出层 --------- #\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "# --------- 评估和训练 ---------- #\n",
    "# 定义交叉熵\n",
    "cross_entropy = - tf.reduce_sum(y_ * tf.log(y_conv))\n",
    "# 使用ADAM梯度最速下降步长是 1e-4 优化损失函数交叉熵\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "# 正确率计算\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(5000):\n",
    "    # 随机梯度下降\n",
    "    batch = dataset.train.next_batch(50)\n",
    "    # 每100次执行一次计算准确率\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "# 评估\n",
    "print(\"test accuracy %g\" % sess.run(accuracy, feed_dict={x: dataset.test.images, y_: dataset.test.labels, keep_prob: 1.0}))\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
