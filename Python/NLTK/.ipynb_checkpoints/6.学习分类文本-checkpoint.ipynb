{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 模式识别是自然语言处理的一个核心任务  \n",
    "    使用机器学习的算法对我们的自然语言处理的任务进行分析和分类\n",
    "    * 监督式分类  \n",
    "        * 建立在训练语料库上的分类我们可以称之为是监督式分类任务\n",
    "            * 训练 : 抽取语料库的特征和标签进行训练算法的模型构建\n",
    "            * 预测 : 抽取带处理的语料的特征进行模型的模式识别生成对应的标签\n",
    "        * 作用\n",
    "            * 文档分类\n",
    "            * 词性标注  \n",
    "                还记得上一章的词性标注中，我们使用了正则表达式标注器，但是如何我确定哪一种正则表达式的特征最能有效的提取正确的标注信息，可以使用对语料的分类来实现分析，选择哪一个后缀对词性标注最有指导性作用(信息量最大)\n",
    "                * 探索上下文语境  \n",
    "                    上下文语境往往会提供对于词性标注的有利依赖\n",
    "            * 序列分类  \n",
    "                1. 我们可以使用所谓的**连续分类策略**,该策略的本质就是我们对第一个次进行词性标注或者分类，然后使用n-grams模型的思路，参考之前的分类结果对之后的单词的标注进行知道，知道所有的单词全部都分类成功(以句子为单位),但是该方法对应的错误就是我们无法对前面的错误进行修正(Brill标注器)\n",
    "                2. 隐马尔科夫模型 : 对词性所有的可能序列进行打分并选取打分结果最高的标注\n",
    "        * 创建分类器  \n",
    "            抽取特征的确定，确定特征值的数据结构\n",
    "            * 朴素贝叶斯分类器  \n",
    "                ```\n",
    "                nltk.NavieBayesClassifier.train([x, y])\n",
    "                ```\n",
    "                * 第一个参数是字典,代表我们的特征，键是特征属性名，值是当前样本对应的特征值\n",
    "                * 第二个参数是样本对应的标签\n",
    "            * 决策树分类器\n",
    "                ```\n",
    "                nltk.DecisionTreeClassifier.train([x, y])\n",
    "                ```\n",
    "            * 最大熵分类器\n",
    "                ```\n",
    "                nltk.MaxentClassifier.train(train_set, algorithm='megam', trace=0)\n",
    "                ```\n",
    "        * 问题\n",
    "            * 过拟合  \n",
    "                1. 这是所有的机器学习算法都会存在一个共同的问题，我们如果选择的特征过多，那么学习算法的泛化效果就会越低  \n",
    "                2. 训练数据过少依然会导致过拟合\n",
    "            * 解决方案\n",
    "                * 对于特征的选取认为是超参数，执行验证算法\n",
    "                * 引入验证集对测试的结果进行在再校验，验证机必须取自训练集不可取自测试集(不能讲任何测试机的信息带入模型中影响学习算法)\n",
    "                * 在验证的时候，我们将错误的信息打印出来进行分析并调整特征的选区方案知道我们希望的标准\n",
    "                * 验证集每次的纠错后都需要重新选取避免学习到验证集的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male\n",
      "0.7710691823899372\n",
      "Most Informative Features\n",
      "                      lw = 'a'            female : male   =     32.1 : 1.0\n",
      "                      lw = 'k'              male : female =     30.8 : 1.0\n",
      "                      lw = 'f'              male : female =     15.3 : 1.0\n",
      "                      lw = 'p'              male : female =     11.2 : 1.0\n",
      "                      lw = 'v'              male : female =     10.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 使用朴素贝叶斯分类器\n",
    "# 以姓名的语料库作为试验标准,特征是名字的最后一个字母对性别的影响\n",
    "# 生成对应的数据标签\n",
    "import nltk\n",
    "\n",
    "def gender_feature(word):\n",
    "    # 返回特征\n",
    "    return {'lw': word[-1]}\n",
    "\n",
    "from nltk.corpus import names\n",
    "import random\n",
    "names = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]\n",
    "random.shuffle(names)    # 打乱顺序\n",
    "\n",
    "# 划分训练集和测试级 90 : 10\n",
    "# feature现在包含特征和标签\n",
    "features = [(gender_feature(n), g) for n, g in names]\n",
    "trainset, testset = features[:7149], features[7149:]    # 9 : 1划分数据集\n",
    "classifier = nltk.NaiveBayesClassifier.train(trainset)    # 生成朴素贝叶斯分类器\n",
    "print(classifier.classify(gender_feature('Neo')))    # 进行预测\n",
    "print(nltk.classify.accuracy(classifier, testset))    # 测试集上的分类正确率\n",
    "# 查看分类器为我们提供的最有效的分类特征标准\n",
    "classifier.show_most_informative_features(5)    # 展示前５个，a结尾的姓名的妇女的概率是男性的33被，这是似然比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tips ：\n",
    "# 在使用分类器的时候，数据中可能存在大量的分类的特征数据集，会占用大量的内存，使用下面的函数可以节约内存\n",
    "from nltk.classify import apply_features\n",
    "# 第一个参数是分类函数，第二个参数是数据集\n",
    "trainset, testset = apply_features(gender_feature, names[:7149]), apply_features(gender_feature, names[7149:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'he', 'the', 'n', 'on', 'ton', 'y', 'ty', 'nty', 'd', 'nd', 'and', 'ry', 'ury', 'id', 'aid', 'ay', 'day', 'an', 'ion', 'f', 'of', 's', \"'s\", \"a's\", 't', 'nt', 'ent', 'ary', 'ed', 'ced', '`', '``', 'o', 'no', 'ce', 'nce', \"'\", \"''\", 'at', 'hat', 'ny', 'any', 'es', 'ies', 'k', 'ok', 'ook', 'ace', '.', 'r', 'er', 'her', 'in', 'end', 'ts', 'nts', 'ity', 've', 'ive', 'ee', 'tee', ',', 'h', 'ch', 'ich', 'ad', 'had', 'l', 'll', 'all', 'ge', 'rge', 'ves', 'se', 'ise', 'ks', 'nks', 'a', 'ta', 'nta', 'or', 'for', 'ner', 'as', 'was', 'ted', 'ber', 'm', 'rm', 'erm', 'en', 'een', 'ged', 'by', 'ior', 'rt', 'urt', 'dge', 'od']\n"
     ]
    }
   ],
   "source": [
    "# 决策树分类器,解决词性标注的问题\n",
    "from nltk.corpus import brown\n",
    "t = []\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    t.extend([word[-1:], word[-2:], word[-3:]])\n",
    "s = nltk.FreqDist(t)    # 生成\n",
    "common = list(s.keys())[:10]    # 提取前100个高频词\n",
    "print(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5337018010274451"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature(word):\n",
    "    features = {}\n",
    "    for i in common:\n",
    "        features['endswith(%s)' % i] = word.lower().endswith(i)    # bool性返回值\n",
    "    return features\n",
    "\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(get_feature(n), g) for n, g in tagged_words]    # 样本输入\n",
    "size = len(featuresets)\n",
    "train_set, test_set = featuresets[:5000], featuresets[500:]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "print(classifier.classify(get_feature('cats')))\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n"
     ]
    }
   ],
   "source": [
    "# 特殊的函数的介绍\n",
    "# nltk.tag.untag(...)将已经词性标注的二元组列表转化成没有标注的列表\n",
    "import nltk\n",
    "brown_tagger_sents = nltk.corpus.brown.tagged_sents(categories='news')\n",
    "for i in brown_tagger_sents[:2]:\n",
    "    print(nltk.tag.untag(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 监督分类举例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', 'START'], ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov', '.', '29', '.'], ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 句子分割\n",
    "# 句子分割是对标点符号的划分的一个学习过程\n",
    "sents = nltk.corpus.treebank_raw.sents()    # 句子黄金分割\n",
    "tokens = []\n",
    "boundaries = []\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    # 每一个sent是一个句子的词列表\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)    # pian\n",
    "    boundaries.append(offset - 1)\n",
    "\n",
    "def get_feature(tokens, i):\n",
    "    return {'next-word-cap': tokens[i+1][0].isupper(), 'preword': tokens[i-1].lower(), 'now': tokens[i], 'pre-word-si-a-char': len(tokens[i-1]==1)}\n",
    "\n",
    "featuresets = [(get_feature(tokens, i), i in boundaries) for i in range(1, len(tokens)-1) if tokens[i] in '.?!']\n",
    "size = int(len(featuresets) * 0.1)\n",
    "trainset, testset = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NavieBayesClassifier.train(trainset)\n",
    "nltk.classifier.accuracy(classifier, testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 评估模式\n",
    "    * 测试集 : \n",
    "        1. 大多数的评估方法是在测试集商队分类效果打分\n",
    "        2. 只使用10%的准则有事并不会产生很好的效果，我们必须综合考量测试集的规模大小\n",
    "        3. 必须要考虑测试集和开发集的相似程度，越相似的话对泛化越无利\n",
    "        4. 尽量不要使用shuffle来进行打乱来筛选测试集，最好将测试集和训练集分别取自不同来源的文件\n",
    "    * 准确度  \n",
    "        对分类的解雇的最简单的性能度量，表示**正确标注分类的比例**,准确度度量的时候还需要考虑高频词的分类情况\n",
    "        ```\n",
    "        nltk.classify.accuracy(classifier, testset)\n",
    "        ```\n",
    "    * 精确率和召回率  \n",
    "        有时候准确度并不能反映出真正的分类的效果，我们有时候关注的重点是被分类为正样例的性能  \n",
    "        * 精确率（P）  \n",
    "            样本中正确的分类中有多是真正正确的分类\n",
    "        * 召回率(R)  \n",
    "            样本中真正正确的分类有多少被分类为是正确的\n",
    "        * F-度量值  \n",
    "            1. 组合精确率和召回率进行的评分\n",
    "            2. 是精确率和召回率的调和平均数\n",
    "                $$\\frac{2 * P * R}{P + R}$$\n",
    "        * 混淆矩阵  \n",
    "            1. 当我们的机器学习的算法的特征标签的数目比较多的时候，我们可以使用混淆矩阵来实现分析\n",
    "            2. 混淆矩阵是一个二维表，元素[i, j]代表正确的标签i被预测为j的次数（比例）\n",
    "            3. 对角线是正确的预测比例，非对角线表示预测错误,.表示0的单元格\n",
    "        * 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 分类模型的机器学习算法\n",
    "    * 决策树\n",
    "        * 信息增益\n",
    "        * 效率(节省时间) ;重用之前的信息加快训练速度\n",
    "        * 过拟合 : 决策树学习道德是训练集的特质而不是问题底层的语言学的模式\n",
    "            * 提前终止树的生长\n",
    "            * 剪枝 : MDL原则(最小描述长度原则)\n",
    "        * 决策树另一个比较明显的缺点就是强迫对特征进行顺序检查，即便特征可能是独立的(很多的特征可能不普遍但是在树的下层会多次重复导致过拟合)\n",
    "        * 不善于利用对分类来说微弱的特征，对所有的分类按照重要性记性了排序不利于\"并行\"对微弱的特征的**共同作用**进行检查和分析\n",
    "    * 朴素贝叶斯分类器\n",
    "    * 最大熵分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863120568566631\n",
      "0.863120568566631\n"
     ]
    }
   ],
   "source": [
    "# 决策树计算熵的大小\n",
    "import nltk\n",
    "import math\n",
    "def entropy(labels):\n",
    "    # 计算香农熵\n",
    "    freq = nltk.FreqDist(labels)\n",
    "    p = [freq.freq(i) for i in list(freq.keys())]\n",
    "    return - sum([i * math.log(i, 2) for i in p])\n",
    "\n",
    "print(entropy([1, 2, 1, 2, 1, 1, 1]))\n",
    "print(entropy([1, 1, 1, 1, 1, 1, 1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 NLTK",
   "language": "python",
   "name": "pythonnltk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
